---
title: "Random Forest Analysis of Heart Failure Dataset"
author: "Sierra Landacre, Tim Leschke, Pamela Mishaw, and Pallak Singh "
format: 
  revealjs:
    smaller: true
    theme: serif
course: STA 6257 - Advanced Statistical Modeling
bibliography: references.bib # file contains bibtex for references
#always_allow_html: true # this allows to get PDF with HTML features
self-contained: true
execute: 
  warning: false
  message: false
editor: 
  markdown: 
    wrap: 72
---

## Random Forest

- Random Forest is a promising machine learning model because it correctly classifies data from large data sets, is resistant to outliers, and is easy to use.
- Random Forest is a group of decision trees (a forest) that are created from identically distributed, independent random samples of data drawn with replacement from the original dataset [@breiman].
- Leo Breiman addresses poor-partitioning, noise, and long compute time with his proposal of the Random Forest model (Breiman 2001).
- An important feature of random Forest is its ability to provide measures of how strong a given variable is associated with a classification result.

## Random Forest Continued 
- Random forest provides valuable insights into feature importance, aiding researchers in understanding which variables contribute most significantly to predictions.
- This attribute is critical in applications where identifying key factors, such as genetic markers in genome-wide association studies, environmental variables in ecological analyses, and heart failure classification is paramount.
- It is an ideal algorithm for studies that involve numerous features such as those found in the biomedical sciences. 
- Its methodology is also more easily conveyed and understood by medical professionals than many other machine learning models, allowing for greater promise in successful real-world implementation.

## Random Forest Success Stories
- Random Forest is used to accurately predict <b>in-hospital mortality</b> for acute kidney injury patients in intensive care [@lin2019predicting].
- Random Forest is used to accurately predict future estimated <b>glomerular filtration rates</b> using electronic medical record data [@zhao2019predicting].
- Random Forest is used to accurately classify gait as displaying or lacking the characteristic of <b>hemiplegia</b> [@luo2020random].
- Random Forest is used to accurately classify persons with type 1 and type 2 <b>diabetes</b> [@rachmawanto].

## Methods

::: columns
::: {.column width="30%"}
![](SingleTree.png){fig-align="left"}

Single classification tree.
:::

::: {.column width="70%"}
![](images/rf%20fig%201-01.png){fig-align="right"}

 <center>Random Forest uses multiple classification trees.</center>
:::
:::

## Methods

::: panel-tabset
### Random Forest Algorithm

![](Algorithm.png){fig-align="center"}

### Formulas

::: columns
::: {.column width="50%"}
The <em>Gini Index</em> is referred to as a measure of node purity
[@james]. It can also be used to measure the importance of each
predictor. The Gini Index is defined by the following formula where K is
the number of classes and ${\hat{p}_{mk}}$ is the proportion of
observations in the <em>m</em>th region that are from the <em>k</em>th
class. A Gini Index of 0 represents perfect purity.

$$D=-\sum_{n=1}^{K} {\hat{p}_{mk}}(1-\hat{p}_{mk})$$
:::

::: {.column width="50%"}
<em>Bagging</em> is the aggregation of the results from each decision
tree. It is defined by the following formula where B is the number of
training sets and $\hat{f}^{*b}$ is the prediction model. Although
bagging improves prediction accuracy, it makes interpreting the results
harder as they cannot be visualized as easily as a single decision tree
[@james].

$${\hat{f}bag(x) = 1/B \sum_{b=1}^{B}\hat{f}^{*b}(x)}$$
:::
:::
:::

## Analysis and Results

-   We ingest the data into R-Studio
-   Perform classification with Random Forest
-   Perform analysis of results

## Software and Hardware Configuration

-   RStudio Pro 2023.12.0, Build 369.pro3
-   Various R libraries
-   RStudio Server running on RHEL9 based virtual machine within a
    VMware VSphere HA cluster. The VM has 50 vCPU's and 196 GB ram
    assigned
-   Hardware includes Dell PowerEdge R750 servers with Dual Xeon Gold
    6338N (32 core) CPUs, 512 GB RAM, and sfp28 25 gbit networking for
    all communications
-   Cluster storage is from an NVMe based Dell SAN.

## Data Used

::: panel-tabset
### Heart Failure Table

![](images/HF%20Table.png){fig-align="center" width="70%"}

### Dataset Information

-   The dataset comes from the Faisalabad Institute of Cardiology and
    the Allied Hospital in Faisalabad Pakistan.
-   There are 299 patient records with 13 features per record.
-   Random Forest model used for predicting heart failure events for
    patients.

![](FaisalabadLogo.png){fig-align="center"}  

:::

## Model Evaluation Metrics

- <b>Out of Bag error rate/accuracy</b> - OOB data is data left unused by a decision tree.
- <b>Confusion Matrix</b> - shows True Positive, False Positive, False Negative and True Negative values to support performance evaluation
- <b>Precision</b> - also known as <em>sensitivity</em> and it represents how many observations labeled positive are actually positive.
- <b>Recall</b> - quantifies how many positive observations are actually predicted as positive.
- <b>F1</b> - harmonic mean of the precision and recall; assesses predictive performance.
- <b>Balanced accuracy</b> - average accuracy of both true-positive and true-negative classes.
- <b>AUC-ROC</b> - area under a curve created by the true positive rate vs. false positive rate.

## Variable Importance Plot

![](VariableImportancePlot.png){fig-align="center"}

## FourFold Plot (Confusion Matrix)

![](FourFoldPlot.png){fig-align="center"}

## Confusion Matrix Heatmap

![](ConfusionMatrixHeatmap.png){fig-align="center"}

## Variable Correlation Heatmap

![](VariableCorrelationHeatmap.png){fig-align="center"}

## ROC - Default Values

![](DefaultROC.png){fig-align="center"}

## trainControl() - Random Selection

![](TrainControl_RandomSelection.png){fig-align="center"}

## trainControl() - Grid Search

![](TrainControl_GridSearch.png){fig-align="center"}

## Variable Importance Plots

::: columns

::: {.column width="50%"}
Default
![](VariableImportancePlot.png){fig-align="center" width="100%"}

:::

::: {.column width="50%"}
Tuned
![](VariableImportancePlot_tuned.png){fig-align="center" width="100%"}
:::

:::

## FourFold Plot - Default vs. Tuned

::: columns
::: {.column width="50%"}

![](FourFoldPlot.png){fig-align="center" width="100%"}
:::

::: {.column width="50%"}
![](FourFoldPlot_tuned.png){fig-align="center" width="100%"}

:::

:::

## Kappa Plot

![](KappaPlot.png){fig-align="center"}

## Tuned vs. Default ROC

![](TunedROC.png){fig-align="center"}

## Model Results

![](ModelResults.png)

-   The testing accuracies of both models are slightly lower than their
    respective training accuracies which indicates overfitting of the
    training dataset.
-   The performance of Model 2 is better than model 1. This is based off
    of the modelâ€™s training and testing accuracies; precision; F1; and
    balanced accuracy as they are all higher than those of the default
    model.

## Conclusion
- The Random Forest model is applied to a heart failure dataset of patients from Faisalabad Pakistan. 
- The Random Forest model provides strong classification results with its default settings. 
- Random Forest makes its decisions in a <em>black box</em>, which means we do not fully understand how is classifies data or which variables are used. 
- We do know from a review of the variable importance plot that <em>time before follow-up visit</em>, <em>serum creatinine levels</em>, and <em>ejection fraction amount</em> are all likely influential in the classification process.

## References
