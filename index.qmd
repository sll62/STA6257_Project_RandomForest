---
title: "Random Forest - Data Science Capstone"
author: "Sierra Landacre, Tim Leschke, Pamela Mishaw, Pallak Singh "
date: '`r Sys.Date()`'
format:
  html:
    code-fold: true
course: STA 6257 - Advanced Statistical Modeling
bibliography: references.bib # file contains bibtex for references
#always_allow_html: true # this allows to get PDF with HTML features
self-contained: true
execute: 
  warning: false
  message: false
editor: 
  markdown: 
    wrap: 72
---

## Introduction
Teaching computers to learn through experience began formally in 1959 when IBM computer gaming pioneer Arthur Samuel coined the term machine learning in support of what is now known as artificial intelligence.[i] Although the modeling of human cognitive function was discussed well before Samuel, he is credited with giving this discipline its identity. 

At the heart of the machine learning algorithm is the decision tree. A decision tree “is a unidirectional, compact, tree-like data structure where each branch point is an attribute-data test that partitions the data rows into two or more branches.”  (@sheppard). An optimized tree is shallow and has few branches.  A decision tree may be used to perform classification or regression analysis to predict a future value given particular features of a subject. Consider, for example, a dataset composed of items that are to be categorized as either land vehicles, watercraft, or aircraft. The first attribute-data test might evaluate if the item flies. If yes, the item is classified as an aircraft. If no, a second attribute-data test will evaluate if the item floats. If yes, the item is classified as a watercraft. If no, the item is classified as a land vehicle. 

A decision tree does not have domain knowledge (@sheppard). It begins as a data structure with a single node that divides the dataset into subsets according to a partitioning algorithm that learns from exposure to training data. Training data is a smaller representation of the main data set, and it contains examples of each class of datapoint found in the main dataset. As the decision tree processes the training data, it grows branches and additional nodes to support the recursive partitioning of subsets into smaller subset until this process terminates with each datapoint being assigned to a category found at each leaf node. This automated machine learning algorithm is challenged by poor-partitioning, noise, and long compute-time (@sheppard).

Noise is another challenge facing machine learning with decision trees. Noise is any impurity found within the dataset. It is random, unpredictable, and unexpected data that disrupts the ability to correctly partition data into categories. It is what some call entropy (@sheppard). The more entropy there is, the more challenging it is to correctly classify each datapoint.

Long compute-time is influenced by both size and complexity. Complex datasets with more categories require a decision tree with more nodes and more branches. Taller and fatter decision trees require more compute-time for processing. Pushing more data through a decision tree has a multiplying effect on the compute-time.
 
Leo Breiman addresses these three issues with his proposal of his Random Forest model (@breiman). His execution of the Random Forest method is a group of decision trees (a forest) that are created from identically distributed, independent random samples of data drawn with replacement from the original dataset (@breiman). Breiman calls this random sample process a bootstrap, which others have re-worded as bootstrapping (https://www.youtube.com/watch?v=v6VJ2RO66Ag). 
 
At each node, a random subset of variables is taken from the array of features and the one that provides the best split is selected for that point. This randomness provides robustness in the presence of misclassifications produced by the minority of inaccurate trees as these are compensated for by the votes of the majority of accurate trees (@breiman). This classification by aggregation is known as ensemble learning and is a proven approach for dealing with bad data (@sheppard).  
 
Because each decision tree is processed with a different random set of data, each grows into a unique structure with a unique classification result. The final classification from Random Forest is a result of aggregating the results from each decision tree; a process Breiman calls bagging (@breiman). This bagging technique addresses both the noise and poor-partitioning of traditional decision trees that results in inaccurate classification. However, by using multiple trees and aggregating the results, the effects of noise and poor-partitioning are minimized, and accuracy is increased (@breiman).  Furthermore, bagging can be used to calculate an “ongoing estimate of the generalization error” which is the error rate of the training set (@breiman).  This can be used to predict how well the model is performing.
 
Accuracy is optimized through a process called boosting.  Boosting is a technique by which more weight is given to the votes that come from decision trees that predict correctly versus those that do not predict correctly. Sheppard states “boosting earned a 1-2 percent overall improvement in the ability to predict the correct outcome while also achieving the percentage more consistently” (@sheppard).
 
Accuracy is also improved through a heuristic known as Ant Colony Optimization (ACO) (@boryczka).  This approach is inspired by ant colony behavior in which individual ants leave traces of pheromone to communicate to other ants regarding the best path to take.  Similarly, the ACO algorithm improves dataset splitting by providing feedback to future splitting choices.  By improving the choice of criteria to split on, one can achieve “maximum homogeneity in the decision classes” which improves overall decision tree accuracy (@boryczka). 
 
Yuan et. al. state the most influential decision tree algorithm is ID3 (@yuan).  ID3 is presented by Quinlan in his 1986 seminal paper Induction of Decision Trees (@quinlan).  This paper lays the foundation for using machine learning to address the bottleneck caused by expert systems.  Quinlan demonstrates it is possible to use decision trees to generate knowledge and solve difficult problems through automation without much computation (@quinlan).  Low-levels of noise do not cause the tree building to “fall over a cliff” (@quinlan).  Furthermore, it is good to train the decision tree with noise if the dataset has noise.
 
Throughput is optimized through parallelization.  Yuan et. al. perform Decision Tree classification with Big Data via a MapReduce algorithm (MapReduce – GA Optimization Decision Tree) that implements parallelization via cloud computing and Hadoop technology. The researchers claim a “higher classification accuracy and shorter running time compared with the traditional decision tree algorithm” (@yuan).
 
 
Since the Random Forest model shows robustness in the face of noisy datasets, it is an ideal algorithm for studies that involve numerous features such as those found in the biomedical sciences. Its methodology is also more easily conveyed and understood by medical professionals than many other machine learning models, allowing for greater promise in successful real-world implementation. 
 
In one such study, researchers create a random forest model to predict in-hospital mortality for acute kidney injury (AKI) patients in the intensive care unit (ICU) (@lin2019predicting). The Random Forest model displays the lowest Brier score (associated with accuracy) and the largest AUROC (associated with discrimination), meaning RF produces the best values for these assessments of the four models, with the second best F1 score and accuracy. The support vector machine (SVM) model had the best accuracy and F1 score but is not statistically significant in difference from the RF results. In another experiment, researchers develop a random forest regression model capable of accurately predicting future estimated glomerular filtration rates (eGFR) using electronic medical record (EMR) data (@zhao2019predicting). This tool allows identification of chronic kidney disease (CKD) in its early stages to more likely prevent progression into end-stage renal disease. The random forest models that are created are found to have accuracies of about 90% for stages 2 and 3 of the CKD and about 80% for stages 4 and 5. Scientists also investigate the accuracy of the Random Forest algorithm for the classification of gait as displaying or lacking the characteristics of hemiplegia, which is the paralysis of one side of the body (@luo2020random).
 
In another study, researchers apply Random Forest to the classification of persons with type 1 and type 2 diabetes.  The conclusion is that “the types of attributes used have an important role in the classification process” (@rachmawanto).  In their study, Random Forest executed with the Abel Vika dataset achieves 100% accuracy, 100% precision, and 100% recall.  In contrast when the Pima Indian Diabetes Dataset is used with Random Forest, the results show at most scores of 75-85% for accuracy, precision, and recall.  The conclusion is that Random Forest excels when the data is noisy and unbalanced. 

Machine learning tools such as decision trees and random forests have recently become a tool used to predict risk factors of type 2 diabetes mellitus. With these tools they can potentially provide data/ results as a supplement in screening for diseases such as type 2 diabetes mellitus. Random forest allows for the generation of multiple classification trees by selecting subsets within a dataset and selecting subsets of predictor variables randomly, which then will aggregate the results of all the tested models which provides a random forest (@esmaily2018comparison). 

Random forest and decision trees both provide important data when used for analyzing data such as the output of the variable importance. The output of variable importance is used to measure the degree of association between given variables and the classification result (@esmaily2018comparison). This provides four measures of variable importance which helps to create good prediction models. Random forest models can provide good prediction models due to efficacy, sensitivity, and specificity (@esmaily2018comparison). 

Utilizing random forest provides many strengths when analyzing data. The strengths are that it doesn’t overfit, is robust to noise, has an internal mechanism called out-of-bag error, provides indices of variable importance, works with mixtures of both continuous and categorical variables, and can be used for cluster analysis and data imputation (@casanova2014application). During the process of creating random forest data two types of randomness will be used. The first is when a “tree” is grown using a bootstrapped version of the training data and the second is added during the growing stage of a “tree” by selecting a random sample of predictors (@casanova2014application).

Random forest is becoming a widely used tool in ecological and population genetics due to the efficiency of being able to analyze thousands of loci and account for nonadditive interactions (@brieuc2018practical). Using this data tool is allowing the study of large genomic data sets to be able to be analyzed as a promising method. Random forest usage has increased over the years for genetic analyses in evolutionary studies (@brieuc2018practical). This tool allows for flexibility in the data types when searching for response and predictor variables. Almost all types of data can be used; however, random forest does not allow missing values for response or predictor variables (@brieuc2018practical).

Some studies use random forest as a tool to compare it to traditional and other reliable machine learning techniques. Random forest was determined to be a clear beneficial tool to use compared to stratification during a statistical comparison of the two approaches (@mascaro2014tale). If random forest is implemented carefully it can be a very useful tool when spatial modeling.

Mohammadamin Moradi's research, detailed in "Random forests for detecting weak signals and extracting physical information: A case study of magnetic navigation," highlights the robustness of random forests in handling noisy feature data and mitigating overfitting through ensemble learning(@moradi2024random). This approach involves training multiple decision trees on subsets of features and data points, aggregating their predictions to enhance accuracy in predicting target variables despite noise. In the context of magnetic navigation in GPS-denied environments, such as aircraft cockpits, random forests excel in detecting and utilizing Earth's anomaly magnetic field amidst interference from cockpit electronics and dominant Earth magnetic components.

The study underscores the independence of random forests from traditional calibration methods like the Tolles-Lawson model, showcasing their ability to predict aircraft positions accurately based solely on feature data from onboard sensors. This autonomous capability simplifies navigation tasks and ensures reliable performance in environments where distinguishing weak signals from noise is crucial for precise positioning. Lai's work demonstrates the versatility and effectiveness of random forests in real-time applications requiring robust signal processing, advancing the field of machine learning in aerospace navigation and beyond(@moradi2024random).

Random forest (RF) has emerged as a powerful machine learning algorithm renowned for its robustness and versatility across various domains. At its core, RF constructs multiple decision trees during training, each trained on a random subset of the data and using a random subset of features. This randomness helps mitigate overfitting and enhances the model's ability to generalize well to unseen data, making it particularly effective in handling noisy datasets commonly found in medical and ecological studies.

One of the key strengths of RF lies in its capability to handle both continuous and categorical data without requiring extensive preprocessing. This flexibility has made it a preferred choice in fields such as genetics and ecological modeling, where datasets often contain diverse types of variables and complex interactions.

Moreover, RF provides valuable insights into feature importance, aiding researchers in understanding which variables contribute most significantly to predictions. This attribute is critical in applications where identifying key factors, such as genetic markers in genome-wide association studies or environmental variables in ecological analyses, is paramount.

In practical applications, RF's scalability and parallelization capabilities are noteworthy, allowing it to efficiently process large datasets using parallel computing techniques like MapReduce. This scalability makes RF suitable for tasks ranging from clinical prediction in healthcare to large-scale genetic analyses, where handling big data is essential for achieving accurate and reliable results.



## Methods



## Analysis and Results

### Data and Visualization
The Random Forest classification method is implemented in the Heart Failure Clinical Records dataset provided by the University of California Irvine Machine Learning Repository (Heart Failure Clinical Records. (2020). UCI Machine Learning Repository). The dataset comes from the Faisalabad Institute of Cardiology and the Allied Hospital in Faisalabad and consists of the medical records of 299 patients who experienced heart failure (Machine learning can predict survival of patients with heart failure from serum creatinine and ejection fraction alone | BMC Medical Informatics and Decision Making | Full Text (biomedcentral.com)). The features are as follows:

```{r}
HF <- data.frame("Feature Name" = c("age","anemia","creatine_phosphokinase","diabetes","ejection_fraction","high_blood_pressure", "platelets","serum_creatinine", "serum_sodium", "sex","smoking","time", "death_event"),
                 "Type" = c("integer","binary", "integer","binary","integer", "binary", "continuous","continuous","integer", "binary","binary","integer","binary"),
                 "Description" = c("age of patient", "decrease of red blood cells or hemoglobin", "level of the CPK enzyme in the blood", "if the patient has diabetes", "percentage of blood leaving the heart at each contraction", "if the patient has hypertension", "platelets in the blood","level of serum creatinine in the blood", "level of serum sodium in the blood", "woman or man", "if the patient smokes or not", "follow-up period in days", "if the patient died during the follow-up period" ))
HF
```

Table source: Heart Failure Clinical Records - UCI Machine Learning Repository
 
The goal is to create and evaluate a model to be used for predicting if a heart failure event may occur given features of a subject.

### Statistical Modeling

### Conclusion


## References
