---
title: "Random Forest Analysis of Heart Failure Dataset"
author: "Sierra Landacre, Tim Leschke, Pamela Mishaw, Pallak Singh "
date: '`r Sys.Date()`'
format:
  html:
    code-fold: true
course: STA 6257 - Advanced Statistical Modeling
bibliography: references.bib # file contains bibtex for references
#always_allow_html: true # this allows to get PDF with HTML features
self-contained: true
execute: 
  warning: false
  message: false
editor: 
  markdown: 
    wrap: 72
---

## Introduction

Correctly classifying a person for being at risk for heart failure is essential for strong preventive health care.  Contributing factors are known to include coronary artery disease, hypertension, obesity, infection, certain medications, and lifestyle choices such as alcohol consumption, smoking, and drug abuse.  Understanding the influences of these competing factors is made easier through unsupervised machine learning which can find patterns and insights in data without explicit human guidance or instruction.  Random Forest is a promising machine learning model because it correctly classifies data from large data sets, is resistant to outliers, and is easy to use.  In this report we validate the application of Random Forest machine learning to support the accurate classification of heart failure among 299 patient profiles.

### Classification with Decision Trees

At the heart of the Random Forest model is the decision tree. A decision tree “is a unidirectional, compact, tree-like data structure where each branch point is an attribute-data test that partitions the data rows into two or more branches.” [@sheppard]. An optimized tree is shallow and has few branches. A decision tree may be used to perform classification or regression analysis to predict a future value given particular features of a subject.

Yuan et. al. state the most influential decision tree algorithm is Iterative Dichotomiser 3 (ID3) [@yuan]. ID3 is presented by Quinlan in his 1986 seminal paper <em>Induction of Decision Trees</em> [@quinlan]. This paper lays the foundation for using machine learning to address the bottleneck caused by expert systems. Quinlan demonstrates it is possible to use decision trees to generate knowledge and solve difficult problems through automation without much computation [@quinlan]. Low-levels of noise do not cause the tree building to “fall over a cliff” and fail [@quinlan]. Furthermore, training the decision tree with noise has been discovered to yield optimal results.  In addition, unbalanced trees tend to perform statistically better than those balanced through stratification [@mascaro2014tale].

A decision tree does not have domain knowledge [@sheppard]. It begins as a data structure with a single node that divides the dataset into subsets according to a partitioning algorithm that learns from exposure to training data. Training data is a smaller representation of the main data set, and it contains examples of each class of datapoint found in the main data set. As the decision tree processes the training data, it grows branches and additional nodes to support the recursive partitioning of subsets into smaller subsets until this process terminates with each datapoint being assigned to a category found at each leaf node. This automated machine learning algorithm is challenged by poor-partitioning, noise, and long compute-time [@sheppard].

Poor-partitioning results in elongated decision trees with impure subsets.  An impure subset occurs when a branch point does not perform a pure classification.  Dirty subsets lead to incorrect classifications, and ultimately to poor heart failure classification in a preventive healthcare workflow.

Noise is another challenge facing machine learning with decision trees. Noise is any impurity found within the dataset. It is random, unpredictable, and unexpected data that disrupts the ability to correctly partition data into categories. It is what some call entropy [@sheppard]. The more entropy there is, the more challenging it is to correctly classify each datapoint. 

Long compute-time is influenced by both size and complexity of the classification dataset. Complex datasets with more categories require a decision tree with more nodes and more branches. Taller and fatter decision trees require more compute-time for processing. Pushing more data through a larger and more complex decision tree has a multiplying effect on the compute-time.

### Classification with Random Forest  

Leo Breiman addresses poor-partitioning, noise, and long compute time with his proposal of the Random Forest model
[@breiman]. Random Forest is a group of decision trees (a forest) that are created from identically distributed, independent random samples of data drawn with replacement from the original dataset [@breiman]. Breiman calls this random sample process a <em>bootstrap</em>, which others have re-worded as <em>bootstrapping</em>. At each node, a random subset of variables is taken from the array of features and the one that provides the best split is selected for that point. This randomness provides robustness in the presence of misclassifications and noise [@breiman], [@moradi2024random], [@casanova2014application]. This classification by aggregation is known as <em>ensemble learning</em> and is a proven approach for dealing with bad data [@sheppard].  Bad data, however, cannot include missing values for response or predictor variables [@brieuc2018practical]. 

Because each decision tree is processed with a different random set of data, each grows into a unique structure with a unique classification result. The final classification from Random Forest is a result of aggregating the results from each decision tree; a process Breiman calls <em>bagging</em> [@breiman]. This bagging technique addresses both the noise and poor-partitioning of traditional decision trees that results in inaccurate classification. By using multiple trees and aggregating the results, the effects of noise and poor-partitioning are minimized, and accuracy is increased [@breiman]. Furthermore, bagging can be used to calculate an “ongoing estimate of the generalization error” which is the error rate of the training set [@breiman]. This can be used to predict how well the model is performing during classification.

Accuracy is optimized through a process called <em>boosting</em>. Boosting is a technique by which more weight is given to the votes that come from decision trees that predict correctly versus those that do not predict correctly. Sheppard states “boosting earned a 1-2 percent overall improvement in the ability to predict the correct outcome while also achieving the percentage more consistently” [@sheppard]. Accuracy is also improved through a heuristic known as Ant Colony Optimization (ACO) [@boryczka]. This approach is inspired by ant colony behavior in which individual ants leave traces of pheromone to communicate to other ants regarding the best path to take. Similarly, the ACO algorithm improves dataset splitting by providing feedback to subsequent splitting choices. By improving the choice of criteria to split on, one achieves “maximum homogeneity in the decision classes” which improves overall decision tree accuracy [@boryczka].

An important feature of random Forest is its ability to provide measures of how strong a given variable is associated with a classification result.  These measures include <em>raw importance score for class 0</em>, <em>raw importance score for class 1</em>, <em>decrease in accuracy</em>, and the <em>Gini Index</em> [@esmaily2018comparison].  Gini Index is explained later.  These scores help users fine-tune the Random Forest method so optimal classification results are achieved.

### Related Work

There are many examples of how Random Forest is used as a successful and scaleable classifier.  In one study, researchers created a Random Forest model to predict in-hospital mortality for acute kidney injury patients in the intensive care unit [@lin2019predicting]. The Random Forest model displays the lowest Brier score (associated with accuracy) and the largest AUROC (associated with discrimination), meaning Random Forest produces the best values for these assessments of the compaired models, with the second best F1 score and accuracy. 

In another experiment, researchers developed a random forest regression model capable of accurately predicting future estimated glomerular filtration rates using electronic medical record data [@zhao2019predicting]. This tool allows identification of chronic kidney disease in its early stages to more likely prevent progression into end-stage renal disease. The random forest models that are created are found to have accuracies of about 90% for stages 2 and 3, and about 80% for stages 4 and 5 of chronic kidney disease. 

Scientists also investigate the accuracy of the Random Forest algorithm for the classification of gait as displaying or lacking the characteristics of hemiplegia, which is the paralysis of one side of the body [@luo2020random].  Researchers achieve a classification accuracy of 95.45%, which they state "is superior than all existing methods."  Because of the natural tendency for Random Forest to be resistant to over-fitting, the researchers "do not have to have careful control over the percentage of patients in the training data" [@luo2020random]. 

When the Random Forest model is applied to the classification of persons with type 1 and type 2 diabetes, the conclusion is that “the types of attributes used have an important role in the classification process” [@rachmawanto]. In their study, Random Forest executed with the Abel Vika dataset achieves 100% accuracy, 100% precision, and 100% recall. In contrast, when the Pima Indian Diabetes Dataset is used, the results show at most scores of 75-85% for accuracy, precision, and recall. The conclusion is that Random Forest excels when the data is noisy and unbalanced as is the case with the first dataset.

In practical applications, Random Forest’s scalability and parallelization capabilities are
noteworthy, allowing it to efficiently process large datasets using parallel computing techniques like MapReduce [@yuan]. This scalability makes random forest suitable for tasks ranging from clinical prediction in healthcare to large-scale genetic analyses, where handling big data is essential for achieving accurate and reliable results.

Random forest provides valuable insights into feature importance, aiding researchers in
understanding which variables contribute most significantly to predictions. This attribute is critical in applications where identifying key factors, such as genetic markers in genome-wide association studies, environmental variables in ecological analyses, and heart failure classification is paramount. Since the Random Forest model shows robustness in the face of noisy datasets, it is an ideal algorithm for studies that involve numerous features such as those found in the biomedical sciences. Its methodology is also more easily conveyed and understood by medical professionals than many other machine learning models, allowing for greater promise in successful real-world implementation. However, in other study types Random Forest is used due to simplicity and diversity, making it one of the commonly used algorithms used for solar energy predictions [@wang2020taxonomy].

In the following sections we explain the Methods pertaining to Random Forest.  This includes the algorithm, measuring node purity with the <em>Gini Index</em>, and aggregation of classification votes from individual decision trees via a processes called <em>bagging</em>.  The Analysis and Results section presents the classification of the Heart Failure Clinical Records dataset processed with R-code and explains how the algorithm is tuned for optimized results [@misc_heart_failure_clinical_records_519].  Lastly, the Conclusion ties all of this work together and crowns Random Forest as a promising machine learning model for classifying people at risk for heart failure.  

## Methods

We assume the reader knows single classification trees.  A Random Forest performs classification via aggregated votes from a group of classification trees built from random data samples.

The algorithm for classification with Random Forest is provided below.   
(Copied, with additions, from [@hastie]).

<pre><span style="text-decoration:overline underline">Random Forest Algorithm                                                                               </span></pre>

<ol type ="1">
<li>For b = 1 to B where B is the number of trees in the forest:
    <ol type ="a">
      <li>Draw a bootstrap sample <b>Z*</b> of size <em>N</em> from the training data.</li>
      <li>Grow a random-forest tree <em>T<sub>b</sub></em> to the bootstrapped data, by recursively repeating the following steps for each terminal node of the tree, until the minimum node size <em>n<sub>min</sub></em> is reached.</li>
        <ol type = "i">
          <li>Select <em>m</em> variables at random from the <em>p</em> variables.</li>
          <li>Pick the best variable/split-point among the <em>m</em>.</li>
          <li>Split the node into two daughter nodes.</li>
        </ol>
      </li>
    </ol>
<li>Output the ensemble of trees.</li>

</ol>

To make a classification prediction at a new point <em>x</em>:

Let <em>C<span>&#770;</span><sub>b</sub>(<em>x</em>)</em> be the class prediction of the <em>b</em>th random-forest tree.  Then <em>C<span>&#770;</span><sub>rf</sub><sup>B</sup>(<em>x</em>) = <em> majority vote</em> {C<span>&#770;</span><sub>b</sub>(<em>x</em>)}<sub>1</sub><sup>B</sup></em>.

<pre><span style="text-decoration:overline">                                                                                                     </span></pre>

Figure 1 shows the generation of a Random Forest.  The uniqueness of each tree, as a result of random data samples, allows the ensemble to avoid misclassification, and improve classification accuracy.

![](Forest.png)
<center>Figure 1. (Modified from TikZ.net,  https://tikz.net/random-forest/, accessed 7/13/2024)</center>\    

The <em>Gini Index</em> is referred to as a measure of node purity [@james].  It can also be used to measure the importance of each predictor.  The Gini Index is defined by the following formula where K is the number of classes and ${\hat{p}_{mk}}$ is the proportion of observations in the <em>m</em>th region that are from the <em>k</em>th class.  A Gini Index of 0 represents perfect purity. 

$$D=-\sum_{n=1}^{K} {\hat{p}_{mk}}(1-\hat{p}_{mk})$$

<em>Bagging</em> is the aggregation of the results from each decision tree.  It is defined by the following formula where B is the number of training sets and $\hat{f}^{*b}$ is the prediction model.  Although bagging improves prediction accuracy, it makes interpreting the results harder as they cannot be visualized as easily as a single decision tree [@james].

$${\hat{f}bag(x) = 1/B \sum_{b=1}^{B}\hat{f}^{*b}(x)}$$


## Analysis and Results

### Data and Visualization  

The Random Forest classification method is implemented in the Heart Failure Clinical Records dataset provided by the University of California Irvine Machine Learning Repository [@misc_heart_failure_clinical_records_519]. The dataset comes from the Faisalabad Institute of Cardiology and the Allied Hospital in Faisalabad Pakistan and consists of the medical records of 299 patients who experienced heart failure [@chicco2020machine].  There are 299 patient records with 13 features per record.  We use this dataset to create and evaluate a model used for predicting heart failure events for patients.  

The features of the Heart Failure Clinical Records dataset are provided in the table below:

```{r, echo=FALSE}
#{r, eval=FALSE, echo=FALSE}
HF <- data.frame("Feature Name" = c("age","anemia","creatine_phosphokinase","diabetes","ejection_fraction","high_blood_pressure", "platelets","serum_creatinine", "serum_sodium", "sex","smoking","time", "death_event"),
                 "Type" = c("integer","binary", "integer","binary","integer", "binary", "continuous","continuous","integer", "binary","binary","integer","binary"),
                 "Description" = c("age of patient", "decrease of red blood cells or hemoglobin", "level of the CPK enzyme in the blood", "if the patient has diabetes", "percentage of blood leaving the heart at each contraction", "if the patient has hypertension", "platelets in the blood","level of serum creatinine in the blood", "level of serum sodium in the blood", "woman or man", "if the patient smokes or not", "follow-up period in days", "if the patient died during the follow-up period" ))
HF
```

<center>Heart Failure Clinical Records - UCI Machine Learning Repository</center>
 <style>
  table.tb { border-collapse: collapse; width:900px; }
  .tb th, .tb td { padding: 5px; border: solid 1px #777; }
  .tb th { background-color: lightblue;}
</style>

<table class="tb" title="Heart Failure Clinical Records - UCI Machine Learning Repository">
  <tr>
    <th>Feature Name</th>
    <th>Description</th>
  </tr>
  <tr>
    <td>Age (integer)</td>
    <td>Age of patient</td>
  </tr>
  <tr>
    <td>Anemia (binary*)</td>
    <td>Decrease of red blood cell or hemoglobin</td>
  </tr>
  <tr>
    <td>Creatine Phosphokinase (integer)</td>
    <td>Level of CPK enzyme in the blood</td>
  </tr>
  <tr>
    <td>Diabetes (binary*)</td>
    <td>If the patient has diabetes</td>
  </tr>
  <tr>
    <td>Ejection Fraction (integer)</td>
    <td>Percentage of blood leaving the heart at each contraction</td>
  </tr>
  <tr>
    <td>High Blood Pressure (binary*)</td>
    <td>If the patient has hypertension</td>
  </tr>
  <tr>
    <td>Platelets (continuous)</td>
    <td>Platelets in the blood</td>
  </tr>
  <tr>
    <td>Serum Creatinine (continuous)</td>
    <td>Level of serum creatine in the blood</td>
  </tr>
  <tr>
    <td>Serum Sodium (integer)</td>
    <td>Level of serum sodium in the blood</td>
  <tr>
    <td>Sex (binary, 0 = female, 1 = male)</td>
    <td>Woman or Man</td>
  </tr>
  <tr>
    <td>Smoking (binary*)</td>
    <td>If the patient smokes or not</td>
  <tr>
    <td>Time (integer)</td>
    <td>Follow-up period in days</td>
  </tr>
  <tr>
    <td>Death Event (binary*)</td>
    <td>If the patient died during the follow-up period</td>
  </tr>
</table>
\* Binary features; 0 = "no" and 1 = "yes".  

Source: [@misc_heart_failure_clinical_records_519]



### Install and Load Packages

The R programming language is used to support data processing and analysis as it is the standard for statistical programming and graphics.

RStudio Pro, 2023.12.0, Build 369.pro3 is used.  Various R libraries and packages are employed including CaTools, RandomForest, Tidyverse, Readr, Dplyr, Party, Caret, Corrplot, Tidyr, Ggplot2, DT, and pRoc. 

The RStudio Server is running on a RHEL9 based virtual machine within a VMware VSphere HA cluster. The VM itself has 50 vCPU's and 196 GB of ram assigned.  

The physical hardware of the cluster nodes are Dell PowerEdge R750 servers with Dual Xeon Gold 6338N (32 core) CPU's, 512 GB of ram, and sfp28 25gbit networking for all communications (6 total sfp28 per server). Cluster storage is from a NVMe based Dell SAN.

```{r}
#{r, echo=FALSE}
install.packages("caTools", repos="http://cran.us.r-project.org")
install.packages("randomForest", repos="http://cran.us.r-project.org")
install.packages("tidyverse", repos="http://cran.us.r-project.org")
install.packages("dplyr", repos="http://cran.us.r-project.org")
install.packages("party", repos="http://cran.us.r-project.org")

library(caTools)
library(randomForest)
library(tidyverse)
library(readr)
library(dplyr)
library(party)
library(caret)
library(corrplot)
```

### Loading the Dataset

The heart failure dataset is loaded from a .csv file.

```{r, echo=FALSE}
heart_df <- read_csv("heart_failure_clinical_records_dataset.csv")
#head(heart_df, 5)
```

### Examining the Data and Preprocessing
```{r}
#{r, echo=FALSE}
cor_matrix <- cor(heart_df)
#corrplot(cor_matrix, method = "color")
# The drawing of the correlation heatmap was moved to may below.  However, some of this
# code is needed earlier, so it was not moved.
```

```{r}
#{r, echo=FALSE}
# What does this code do?
heart_df %>% summarise(across(everything(), ~ sum(is.na(.))))
```


```{r}
#{r, echo=FALSE}
# What does this code do?
# does it need to be with the code above?  If so, should it be one code chunk?
heart_df %>% count(DEATH_EVENT)
```


```{r}
#{r, echo=FALSE}
# What does this code do?
# does it need to be with the code above?  If so, should it be one code chunk?
heart_df$DEATH_EVENT = as.factor(heart_df$DEATH_EVENT)
```

The dataset is examined for null values, and no null values are found. 

The number of negative events (non-heart-failure events) are found to be about double the number of positive events (heart-failure events).  This imbalance is not considered significant as the Random Forest algorithm is robust enough to tolerate this asymmetry. 

The DEATH_EVENT feature is treated as a factor since it is a binary dependent variable.

### Metrics Used for Model Evaluation

There are a number of metrics available for assessing a classification model's performance. The metrics used are described in the following text:

* **OOB error rate/accuracy**: The Out-of-Bag data is the data left unused by an individual predictor (decision tree) after a bootstrap sample is taken from the original data set. The OOB data is used for internal validation– the prediction error rate is taken after running the “unseen” data through each tree and averaged to provide the overall OOB error rate. [@breiman] A model's accuracy is calculated by subtracting the error rate from 100.

* A **confusion matrix** shows the values of True Negative, False Negative, False Positive, and True Positive predictions produced by a model. Several performance metrics for classification algorithms like the random forest method are presented by the <em>confusionMatrix</em> function and derived from the values provided in confusion matrices. Here we focus on precision, recall, the F1 score, and balanced accuracy. 
  + **Precision** is also known as the sensitivity and is calculated by dividing the number of true positive predictions by the sum of the true positives and false positives. This metric quantifies how many of the observations labeled as positive are actually positive [@raschka2022machine]. 
  + **Recall** is also known as the true positive rate — this is calculated by dividing the number of true positive predictions by the the sum of the false negatives and true positives. It quantifies how many of the positive observations have been predicted as positive [@raschka2022machine]. 
  + The **F1 score** is the harmonic mean of the precision and recall and so used to assess predictive performance. [@raschka2022machine] 
  + **Balanced accuracy** is the average accuracy of classification for both classes. It is given by the average of the true positive rate and the true negative rate. It aids in reducing the effects of class imbalance in the data set so that accuracy of predicting the dominant class does not conceal the classification accuracy for the minority class [@brodersen2010balanced].

*  **AUC-ROC**: Area under the receiver operating characteristic curve. This is the model’s true positive rate (on the Y-axis) is plotted against the false positive rate (on the X-axis) using varying classification thresholds [@1388242]. An ideal ROC curve would have a high true positive rate and low false positive rate, falling into the upper left area of the graph. This equates to a value close to 1. A diagonal line on the graph represents an AUC of 0.5, equivalent to random guessing of the classifications [@raschka2022machine].

### Running the Random Forest Model

The patient health records dataset is split into a training set (80%) and a testing set (20%). An id column is created to assist in the split.

```{r}
# Split the data, 80% goes to training and 20% to testing

set.seed(1)

samp <- sample(nrow(heart_df), 0.8 * nrow(heart_df))

heart_train <- heart_df[samp, ]

heart_test <- heart_df[-samp, ]

head(heart_train)
head(heart_test)
```

The training and testing sets are inspected.

```{r}
dim(heart_train)

dim(heart_test)
```

239 subjects were randomly placed into the training set and the remaining 50 were designated for testing.

The random forest model is then run on the dataset using the default settings.

```{r}
model <- randomForest(
  formula = DEATH_EVENT~.,
  data = heart_train,
  importance = TRUE
)

model
```

```{r}
importance(model)
```

```{r}
randomForest::varImpPlot(model, 
                         sort=FALSE, 
                         main="Variable Importance Plot")
```

The Variable Importance Plot above is a visual representation of the importance of each variable in our model.  Circles are plotted on a scale at a location that is influenced by their accuracy and Gini Coefficient.  The resulting value provides an indication of how useful a variable is for predicting an outcome.  The higher the value, the greater the influence.

In the mean Decrease Accuracy plot, <en>time</em> has the highest value of approximately 40.  Because time is the number of days between patient visits to their health care provider, it seems the longer a patient waits for a follow-up visit, the more at risk they are for heart failure.

<em>Serum_creatinine</em> and <em>ejection_fraction</em> are tied for second at about 15 within the Mean Decrease Accuracy plot.  Serum creatinine is a waste product of muscles and is usually filtered out of one’s blood by a healthy kidney.  High serum_creatinine is often an indicator of unhealthy kidneys.  

<Em>Ejection fraction</em> is the amount of blood a heart pumps each time it beats.  It is calculated by dividing the <em>stroke volume</em> by the <em>end-diastolic volume</em>, all multiplied by 100.  An ejection fraction of 50-70% is considered healthy.  A low ejection fraction rate is an  indicator of heart failure and/or heart disease.    

Although the numbers are slightly lower, <em>time</em>, <em>Serum_creatinine</em>, and <em>ejection_fraction</em> are also the top three variables for predicting heart failure according to the Mean Decrease Gini Index plot. 

The default random forest model produced an Out-of-the-Bag (OOB) estimate of error rate of 17.99%.  There is a higher class error rate in the "positive" (death event occurrence) class than the "negative" (no death occurrence) class, with 36.0 % versus 9.76% rate, respectively.

```{r}
prediction <- predict(model, newdata = heart_test)


cf <- confusionMatrix(prediction, heart_test$DEATH_EVENT, positive = "1", mode = "everything")
cf
```


```{r}
# Produce plot of the confusion matrix

fourfoldplot(as.table(cf),color=c("yellow","blue"),main = "Confusion Matrix for Random Forest -- Default Settings")
```

The visualization above is called a <em>FourFold Plot</em> and it is used to represent a 2 x 2 contingency table to support evaluation.  In this case the contingency table is the confusion matrix resulting from our model.  The top left square shows the algorithm correctly classifies ‘heart failure’ 36 times, and the bottom right shows the algorithm correctly classifies ‘not heart failure’ 17 times. The errors are shown in the remaining squares; the top right shows the algorithm incorrectly classifies ‘heart failure’ as FALSE (not heart failure) 4 times when it should be classified as TRUE, and the bottom left shows the algorithm incorrectly classifies ‘not heart failure’ as TRUE (heart failure) 3 times when it should be classified as FALSE. The colors are used to dual-encode correct prediction (blue) and incorrect prediction (yellow).  The area of each quarter-circle is determined by a radius of $\sqrt{f_{ij}}$ where ${f_{ij}}$ is the cell frequency [@fourfold].  The message being portrayed is that 53 classifications are correct, and only 7 are incorrect.  This yields and accuracy of 88.33%, as mentioned previously.



```{r}
# The code below creates a heatmap from the values provided in the confusion matrix.
# Notice the values are hard-coded and do not come from an existing data structure.
# https://stackoverflow.com/questions/7421503/how-to-plot-a-confusion-matrix-using-heatmaps-in-r

library("dplyr")
library("tidyr")

# Loading the confusion matrix
hm <- readr::read_delim("y FALSE TRUE
FALSE 3 17
TRUE 36 4", delim = " ")

hm <- hm %>% gather(x, value, "FALSE":"TRUE")


library("ggplot2")
ggplot(hm, aes(x=x, y=y, fill=value)) + geom_tile() +
  scale_x_discrete(label = c("TRUE", "FALSE")) +
  labs(title="Confusion Matrix Heatmap", x="Predicted Values", y="Actual Values") +
  geom_text(aes(label=value), color="black") +
  scale_fill_distiller(palette="Blues", direction = 1)
```

The Confusion Matrix Heatmap above shows an alternate way to visualize the performance of the algorithm regarding its ability to correctly predict an outcome.  Just like with the FourFold Plot, the top left square shows the algorithm correctly classified 'heart failure' 36 times, and the bottom right shows the algorithm correctly classified 'not heart failure' 17 times.  The errors are shown in the remaining squares; the top right shows the algorithm incorrectly classified 'heart failure' as FALSE (not heart failure) when it should have been classified as TRUE, and the bottom left shows the algorithm incorrectly classified 'not heart failure' as TRUE (heart failure) when it should have been classified as FALSE.  The darker color gradient helps one perceive the relative amount recorded in each of the four areas.  


```{r}
# Draws the corelation heatmap
corrplot(cor_matrix, method = "color")
```

The Variable Correlation Heatmap above shows the correlation between multiple model variables as a color-coded matrix.  Model variables that are positively correlated have a blue square at their intersection and negatively correlated model variables have a red square at their intersection.  The color saturation represents the intensity of the correlation (blue) or lack of correlation (red).  For example, the intersection of serum_creatinine along the left side with age along the top results in a light blue square representing a slightly positive correlation between these two model variables.  On the other hand, serum_creatinine along the left side intersects with serum_sodium along the top resulting in a slightly red square representing a slightly negative correlation between these two model variables.  Other model  variables like diabetes and creatinine_phosphokinase intersect with no color, which means there is no positive or negative correlation between these two model variables.  

```{r}
# List the predicted classes produced by the model
table(prediction)
```

The above table needs to be explained.

```{r}
# Create a list of the prediction results alongside the actual DEATH_EVENT values from the test set
results <- cbind(prediction, heart_test$DEATH_EVENT)
colnames(results)<-c('pred','real')

results<-as.data.frame(results)

library(DT)
datatable(results)
```

The above table needs to be explained.

```{r}
# Calculate the classification accuracy of the model
sum(prediction==heart_test$DEATH_EVENT) / nrow(heart_test)
```

Is this redundant?  Do we need this?  I think we can keep it if we explain it.

```{r}
# Create prediction matrix and utilize it to generate ROC values that can be graphed
prediction <- predict(model, heart_test, type = "prob")
prediction
library(pROC)
ROC_rf <- roc(heart_test$DEATH_EVENT, prediction[,2])

# Generate Area Under Curve (AUC) for ROC curve (higher -> better)
ROC_rf_auc <- auc(ROC_rf)
```

Should the above be explained?

```{r}
# Plot ROC curves
plot(ROC_rf, col = "green", main = "ROC For Random Forest, Default Settings")

# Print the performance of each model
paste("Accuracy % of default random forest: ", mean(heart_test$DEATH_EVENT == round(prediction[,2], digits = 0)))
paste("Area under curve of default random forest: ", ROC_rf_auc)
```

The above needs to be explained.

The classification accuracy with the default hyperparameters for the randomForest package comes to 88.3%.

```{r}
# Alternative way to create RF model with default parameters
control <- trainControl(method="repeatedcv", number=10, repeats=3)
seed <- 1
metric <- "Accuracy"
set.seed(seed)
mtry <- sqrt(ncol(heart_train))
tunegrid <- expand.grid(.mtry=mtry)
rf_default <- train(DEATH_EVENT~., data=heart_train, method="rf", metric=metric, tuneGrid=tunegrid, trControl=control)
print(rf_default)
```

This alternative implementation of the random forest algorithm produced a classification accuracy of 83.5% and kappa is 0.607 which is calculated by the observed and expected accuracy.

### Tuning the Model

```{r}
# Implement a random search for tuned mtry
control <- trainControl(method="repeatedcv", number=10, repeats=3, search="random")
set.seed(seed)
mtry <- sqrt(ncol(heart_train))
rf_random <- train(DEATH_EVENT~., data=heart_train, method="rf", metric=metric, tuneLength=13, trControl=control)
print(rf_random)
plot(rf_random)
```

The random search found the model's accuracy to be highest at mtry = 3. This was the default value for our randomForest model.

```{r}
# Use gridsearch to determine best mtry value
control <- trainControl(method="repeatedcv", number=10, repeats=3, search="grid")
set.seed(seed)
tunegrid <- expand.grid(.mtry=c(1:13))
rf_gridsearch <- train(DEATH_EVENT~., data=heart_train, method="rf", metric=metric, tuneGrid=tunegrid, trControl=control)
print(rf_gridsearch)
plot(rf_gridsearch)
```

The grid search suggests that the optimal mtry value is 4. The randomForest model that was created earlier can now be tested with this new mtry.

```{r}
model2 <- randomForest(
  formula = DEATH_EVENT~.,
  data = heart_train,
  mtry = 4,
  importance = TRUE
)

model2
```

```{r}
importance(model2)
randomForest::varImpPlot(model2, 
                         sort=FALSE, 
                         main="Variable Importance Plot")
```

```{r}
prediction2 <- predict(model2, newdata = heart_test)


cf2 <- confusionMatrix(prediction2, heart_test$DEATH_EVENT, positive = "1", mode = "everything")
cf2
```

```{r}
fourfoldplot(as.table(cf2),color=c("yellow","blue"),main = "Confusion Matrix for Random Forest -- Tuned mtry")
```

After changing the mtry hyperparameter to 4, the OOB estimate of error rate decreased slightly to 17.15%. The "positive" classification error did not improve– it worsened slightly, coming to about 30.7%. The "negative" classification error also increased slightly at a new value of about 11.0%.

Next, the number of trees (ntree) is tested over the range 500 (default) to 1500 to see if accuracy can be improved by changes in this hyperparameter.

```{r}
x <- heart_df[,1:12]
y <- heart_df[,13]

control <- trainControl(method = 'repeatedcv',
                        number = 10,
                        repeats = 3,
                        search = 'grid')
#create tunegrid
tunegrid <- expand.grid(.mtry = c(sqrt(ncol(heart_train))))
modellist <- list()

#train with different ntree parameters
for (ntree in seq(500,1500,100)){
  set.seed(1)
  fit <- train(DEATH_EVENT~.,
               data = heart_train,
               method = 'rf',
               metric = 'Accuracy',
               tuneGrid = tunegrid,
               trControl = control,
               ntree = ntree)
  key <- toString(ntree)
  modellist[[key]] <- fit
}

#Compare results
results <- resamples(modellist)
summary(results)
```

```{r}
dotplot(results)
```

The results suggest that the optimal ntree value is 600. This new ntree values is tested, along with the tuned mtry value of 4, using the randomForest model from before.

```{r}
model3 <- randomForest(
  formula = DEATH_EVENT~.,
  data = heart_train,
  mtry = 4,
  ntree = 600,
  importance = TRUE
)

model3
```

```{r}
importance(model3)
randomForest::varImpPlot(model3, 
                         sort=FALSE, 
                         main="Variable Importance Plot")
```

The OOB value slightly decreased to 16.32% while the classification error rates returned to those generated by the default settings for randomForest.

```{r}
prediction3 <- predict(model3, newdata = heart_test)


cf3 <- confusionMatrix(prediction3, heart_test$DEATH_EVENT, positive = "1", mode = "everything")
cf3
```

```{r}
fourfoldplot(as.table(cf3),color=c("yellow","blue"),main = "Confusion Matrix for Random Forest -- Tuned (mtry and ntree)")
```

```{r}
table(prediction2)

```

```{r}
# Create a list of the prediction results alongside the actual DEATH_EVENT values from the test set 
results2 <- cbind(prediction2, heart_test$DEATH_EVENT) 
colnames(results2)<-c('pred','real')  
results2<-as.data.frame(results2)  
datatable(results2)
```

```{r}
# Calculate the classification accuracy of the model 
sum(prediction2==heart_test$DEATH_EVENT) / nrow(heart_test)
```

```{r}
prediction2 <- predict(model3, heart_test, type = "prob") 
prediction2 
ROC_rf2 <- roc(heart_test$DEATH_EVENT, prediction2[,2])  
# Area Under Curve (AUC) for each ROC curve (higher -> better) 
ROC_rf2_auc <- auc(ROC_rf2)
```

```{r}
# plot ROC curves 
plot(ROC_rf2, col = "red", main = "ROC For Random Forest, Default Settings")
lines(ROC_rf, col = "green")
# print the performance of each model 
paste("Accuracy % of default random forest: ", mean(heart_test$DEATH_EVENT == round(prediction[,2], digits = 0)))
paste("Area under curve of default random forest: ", ROC_rf_auc)

paste("Accuracy % of tuned random forest: ", mean(heart_test$DEATH_EVENT == round(prediction2[,2], digits = 0))) 
paste("Area under curve of tuned random forest: ", ROC_rf2_auc)
```

The accuracy and area under the receiver operating characteristic curve (AUC-ROC) are higher for the model produced using the default hyperparameters (88.3% and 0.932, respectively) compared to the "tuned" model (86.7% and 0.926, respectively).

### Model Results

<center>Model Results</center>
 <style>
  table.tb { border-collapse: collapse; width:900px; }
  .tb th, .tb td { padding: 5px; border: solid 1px #777; }
  .tb th { background-color: lightblue;}
</style>

<table class="tb" title="Model Results">
  <tr>
    <th>Model</th>
    <th>OOB Error/Training Acc</th>
    <th>Testing Acc</th>
    <th>Precision</th>
    <th>Recall</th>
    <th>F1 Score</th>
    <th>Balanced Accuracy</th>
    <th>AUC-ROC</th>
  </tr>
  <tr>
    <td>Model 1 (default)</td>
    <td>17.99%/82.01%</td>
    <td>0.8833</td>
    <td>0.8500</td>
    <td>0.8095</td>
    <td>0.8293</td>
    <td>0.8663</td>
    <td>0.9328</td>
  </tr>
  <tr>
    <td>Model 2 (Tuned mtry)</td>
    <td>16.74%/83.26%</td>
    <td>0.8667</td>
    <td>0.8095</td>
    <td>0.8095</td>
    <td>0.8095</td>
    <td>0.8535</td>
    <td>N/A</td>
  </tr>
  <tr>
    <td>Model 3 (Tuned mtry, ntree)</td>
    <td>16.32%/83.68%</td>
    <td>0.8833</td>
    <td>0.8182</td>
    <td>0.8571</td>
    <td>0.8372</td>
    <td>0.8773</td>
    <td>0.9255</td>
  </tr>
</table>  

When looking at the default model (model 1) we got an OOB error of error rate of 17.99%, which translates to a training accuracy of 82.01%. The testing accuracy was 0.8833 (88.33%). The precision value came to 85% which shows that the model is correct around 85% of the time. The recall is 80.9% which shows that this model identifies actual positive cases about 80.9% of the time. The F1 score is 82.9% which is the combination of the model’s precision and recall. Lastly the balanced accuracy is 86.6% which is the average of the models true positive and true negative.  	 

The second model (model 2) the OOB error is 16.74% which translates to a training accuracy of 83.26%. The testing accuracy came to 0.8667 (86.67%). The precision value came to 0.8095 which shows that this model is correct around 80.95% of the time. The recall is also 0.8095 which shows that the model identifies actual positive cases around 80.95% of the time. The F1 score is 80.95% which is the combination of the model’s precision and recall. Lastly the balanced accuracy is 85.35% which is the average of the models true positive and true negative. 	 

In our final model (model 3) the OOB error is 16.32% which translates to a training accuracy of 83.68%. The testing accuracy was 0.8833 (88.33%). We have a precision value of 0.8182 which shows that this model is correct around 81.82% of the time. The recall is 0.8571 which shows that the model identifies actual positive cases around 85.71% of the time. The F1 score is 83.72% which is the combination of the model’s precision and recall. Lastly the balanced accuracy is 87.73%, which is the average of the models true positive and true negative. 

Out of the three models tested model 3 is best. This is going off the model’s F1, balanced accuracy, and recall percentages. The F1 score is 83.72% and the balanced accuracy is 87.73% are the highest percentages within the three models tested. The F1 provides the overall performance of the model since it is a combination of the precision and recall percentages. The balanced accuracy is the average accuracy of classifications for both classes. Lastly, although the precision decreased compared to the non-tuned model (Model 1), the third model's recall is the highest which is important in cases such as heart failure prediction-- a higher recall translates to a decrease in false negatives at the expense of a decrease in precision (which equates to an increase in false positives). It is typically more important to not fail to detect positive cases than to have a 'false alarm' diagnosis.

The output of a Random Forest model is influenced by subsample size, the number of variables assessed at each node, the maximum nodesize of each leaf, and the number of predictors generated [@scornet2017tuning]. In R, the defaults are a subsample size of sqrt(𝑑) for classification (where d is the total number of dimensions), the generation of 500 predictors, and a nodesize of 5 [@scornet2017tuning]. In one comparison study, the balanced accuracy was found to be optimal at 1300 trees and 3 nodes in R and led to an increase of 0.09 (9%) in balanced accuracy as well as a 0.14 (14%) in the AUC-ROC compared to the values generated by the default hyperparameters [@lotsch2022biomedical].The results suggest the importance of tuning the hyperparameters of random forest algorithms, particularly in the case of exploratory analysis that has initial unfavorable accuracy. In the case of this dataset, the model with the tuned hyperparameters (Model 3) provided better performance according to training/testing accuracy, recall, F1 score, and balanced accuracy, which suggests the importance of considering hyperparameter tuning when constructing random forest models.

The variable importance plots for each model reveal that the feature with the greatest influence on prediction is "time," which is . The variables "ejection_fraction" and "serum_creatinine" provided the second-most influence. The variables "age" and "serum_sodium" also showed relatively significant importance. 

## Conclusion

In conclusion, the application of Random Forest classification on the Heart Failure Clinical Records dataset yielded promising results in predicting heart failure events. By leveraging a diverse range of patient data including age, medical conditions, physiological indicators, and lifestyle factors, the model achieved an accuracy of approximately 86.7% on the testing set. This indicates a strong ability to discern between patients who are likely to experience heart failure and those who are not.

The initial recognition of class imbalance and subsequent hyperparameter tuning were critical steps in enhancing the model's performance. Adjusting the mtry parameter to 4 through cross-validation improved sensitivity, thereby reducing errors in predicting positive instances of heart failure. Moreover, experiments with varying tree numbers reaffirmed that the default setting of 500 trees was optimal, demonstrating stable accuracy levels without significant gains beyond this point.

Overall, the tuned Random Forest model not only addressed the dataset's inherent challenges but also showcased robustness in handling class imbalance and leveraging multiple features effectively. Its high sensitivity (89.7%) and specificity (80.9%) underscore its reliability in clinical applications, offering potential for early detection and personalized management of heart failure. Moving forward, further validation on larger datasets could solidify its role as a valuable tool in cardiovascular risk assessment and patient care.


## References
