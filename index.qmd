---
title: "Random Forest - Data Science Capstone"
author: "Sierra Landacre, Tim Leschke, Pamela Mishaw, Pallak Singh "
date: '`r Sys.Date()`'
format:
  html:
    code-fold: true
course: STA 6257 - Advanced Statistical Modeling
bibliography: references.bib # file contains bibtex for references
#always_allow_html: true # this allows to get PDF with HTML features
self-contained: true
execute: 
  warning: false
  message: false
editor: 
  markdown: 
    wrap: 72
---

## Introduction
Teaching computers to learn through experience began formally in 1959 when IBM computer gaming pioneer Arthur Samuel coined the term machine learning in support of what is now known as artificial intelligence.[i] Although the modeling of human cognitive function was discussed well before Samuel, he is credited with giving this discipline its identity. 

At the heart of the machine learning algorithm is the decision tree. A decision tree ‚Äúis a unidirectional, compact, tree-like data structure where each branch point is an attribute-data test that partitions the data rows into two or more branches.‚Äù  (@sheppard). An optimized tree is shallow and has few branches.  A decision tree may be used to perform classification or regression analysis to predict a future value given particular features of a subject. Consider, for example, a dataset composed of items that are to be categorized as either land vehicles, watercraft, or aircraft. The first attribute-data test might evaluate if the item flies. If yes, the item is classified as an aircraft. If no, a second attribute-data test will evaluate if the item floats. If yes, the item is classified as a watercraft. If no, the item is classified as a land vehicle. Figure 1 shows our rudimentary decision tree.

<Insert Figure 1 here>

A decision tree does not have domain knowledge (@sheppard). It begins as a data structure with a single node that divides the dataset into subsets according to a partitioning algorithm that learns from exposure to training data. Training data is a smaller representation of the main data set, and it contains examples of each class of datapoint found in the main dataset. As the decision tree processes the training data, it grows branches and additional nodes to support the recursive partitioning of subsets into smaller subset until this process terminates with each datapoint being assigned to a category found at each leaf node. This automated machine learning algorithm is challenged by poor-partitioning, noise, and long compute-time (@sheppard).

Poor-partitioning results in elongated decision trees with impure subsets.  An impure subset occurs when a branch point does not perform a pure classification.  Consider the decisions captured in Figure 1 and suppose the classification questions are asked in reverse order.  If the first question is does the item float?, both boats and seaplanes are incorrectly grouped together.  This results in two groups; cars with airplanes (non-seaplanes), and boats with airplanes (seaplanes).  Because airplanes are found in each subset, the subsets are impure.

Noise is another challenge facing machine learning with decision trees. Noise is any impurity found within the dataset. It is random, unpredictable, and unexpected data that disrupts the ability to correctly partition data into categories. It is what some call entropy (@sheppard). The more entropy there is, the more challenging it is to correctly classify each datapoint.

Long compute-time is influenced by both size and complexity. Complex datasets with more categories require a decision tree with more nodes and more branches. Taller and fatter decision trees require more compute-time for processing. Pushing more data through a decision tree has a multiplying effect on the compute-time.
 
Leo Breiman addresses these three issues with his proposal of his Random Forest model (@breiman). His execution of the Random Forest method is a group of decision trees (a forest) that are created from identically distributed, independent random samples of data drawn with replacement from the original dataset (@breiman). Breiman calls this random sample process a bootstrap, which others have re-worded as bootstrapping (https://www.youtube.com/watch?v=v6VJ2RO66Ag). 
 
At each node, a random subset of variables is taken from the array of features and the one that provides the best split is selected for that point. This randomness provides robustness in the presence of misclassifications produced by the minority of inaccurate trees as these are compensated for by the votes of the majority of accurate trees (@breiman). This classification by aggregation is known as ensemble learning and is a proven approach for dealing with bad data (@sheppard).  
 
Because each decision tree is processed with a different random set of data, each grows into a unique structure with a unique classification result. The final classification from Random Forest is a result of aggregating the results from each decision tree; a process Breiman calls bagging (@breiman). This bagging technique addresses both the noise and poor-partitioning of traditional decision trees that results in inaccurate classification. However, by using multiple trees and aggregating the results, the effects of noise and poor-partitioning are minimized, and accuracy is increased (@breiman).  Furthermore, bagging can be used to calculate an ‚Äúongoing estimate of the generalization error‚Äù which is the error rate of the training set (@breiman).  This can be used to predict how well the model is performing.
 
Accuracy is optimized through a process called boosting.  Boosting is a technique by which more weight is given to the votes that come from decision trees that predict correctly versus those that do not predict correctly. Sheppard states ‚Äúboosting earned a 1-2 percent overall improvement in the ability to predict the correct outcome while also achieving the percentage more consistently‚Äù (@sheppard).
 
Accuracy is also improved through a heuristic known as Ant Colony Optimization (ACO) (@boryczka).  This approach is inspired by ant colony behavior in which individual ants leave traces of pheromone to communicate to other ants regarding the best path to take.  Similarly, the ACO algorithm improves dataset splitting by providing feedback to future splitting choices.  By improving the choice of criteria to split on, one can achieve ‚Äúmaximum homogeneity in the decision classes‚Äù which improves overall decision tree accuracy (@boryczka). 
 
Yuan et. al. state the most influential decision tree algorithm is ID3 (@yuan).  ID3 is presented by Quinlan in his 1986 seminal paper Induction of Decision Trees (@quinlan).  This paper lays the foundation for using machine learning to address the bottleneck caused by expert systems.  Quinlan demonstrates it is possible to use decision trees to generate knowledge and solve difficult problems through automation without much computation (@quinlan).  Low-levels of noise do not cause the tree building to ‚Äúfall over a cliff‚Äù (@quinlan).  Furthermore, it is good to train the decision tree with noise if the dataset has noise.
 
Throughput is optimized through parallelization.  Yuan et. al. perform Decision Tree classification with Big Data via a MapReduce algorithm (MapReduce ‚Äì GA Optimization Decision Tree) that implements parallelization via cloud computing and Hadoop technology. The researchers claim a ‚Äúhigher classification accuracy and shorter running time compared with the traditional decision tree algorithm‚Äù (@yuan).
 
[I do not understand the next paragraph.  What is nodesize?  Why do we care about the default values in R?  Do we need all of these details to make the point you are trying to make?  Perhaps the following paragraph can be re-written.  ‚Äì Tim]
 
The output of a Random Forest model is influenced by subsample size, the number of variables assessed at each node, the maximum nodesize of each leaf, and the number of predictors generated (@scornet2017tuning). In R, the defaults are a subsample size of ùëë for classification (where d is the total number of dimensions), the generation of 500 predictors, and a nodesize of 5 (@scornet2017tuning). One form of enhancing the performance of the Random Forest algorithm is seen with boosting (@scornet2017tuning). In one study researchers performed, in R and Python, the creation of random forests using a range of trees, from 100 to 1500 with steps of 100 (L√∂tsch, J√∂rn, and Benjamin Mayer). The number of nodes was also tested in the range of 1 to 8. The balanced accuracy (0.65, 95% CI = 0.53 to 0.77) and ROC-AUC(0.73, 95% CI = 0.59 to 0.85) were found to be optimal at 1300 trees and 3 nodes in R, compared to a balanced accuracy of 0.56 (95% CI = 0.42 to 0.68) and an AUC-ROC of 0.59 (95% CI = 0.45 to 0.72) produced by the default hyperparameters. In Python, the best results were obtained with 700 trees and a limit of one split, with a balanced accuracy of 0.67 (95% CI = 0.55 to 0.78) and an AUC-ROC of 0.72 (95% CI = 0.58 to 0.85) compared to a balanced accuracy of 0.55 (95% CI = 0.43 to 0.67) and an AUC-ROC of 0.58(95% CI = 0.45 to 0.7) produced by the un-tuned Python algorithm. The results suggest the importance of tuning the hyperparameters of random forest algorithms, particularly in the case of exploratory analysis that has initial unfavorable accuracy.
 
Since the Random Forest model shows robustness in the face of noisy datasets, it is an ideal algorithm for studies that involve numerous features such as those found in the biomedical sciences. Its methodology is also more easily conveyed and understood by medical professionals than many other machine learning models, allowing for greater promise in successful real-world implementation. 
 
In one such study, researchers create a random forest model to predict in-hospital mortality for acute kidney injury (AKI) patients in the intensive care unit (ICU) (@lin2019predicting). The Random Forest model displays the lowest Brier score (associated with accuracy) and the largest AUROC (associated with discrimination), meaning RF produces the best values for these assessments of the four models, with the second best F1 score and accuracy. The support vector machine (SVM) model had the best accuracy and F1 score but is not statistically significant in difference from the RF results. In another experiment, researchers develop a random forest regression model capable of accurately predicting future estimated glomerular filtration rates (eGFR) using electronic medical record (EMR) data (@zhao2019predicting). This tool allows identification of chronic kidney disease (CKD) in its early stages to more likely prevent progression into end-stage renal disease. The random forest models that are created are found to have accuracies of about 90% for stages 2 and 3 of the CKD and about 80% for stages 4 and 5. Scientists also investigate the accuracy of the Random Forest algorithm for the classification of gait as displaying or lacking the characteristics of hemiplegia, which is the paralysis of one side of the body (@luo2020random).
 
In another study, researchers apply Random Forest to the classification of persons with type 1 and type 2 diabetes.  The conclusion is that ‚Äúthe types of attributes used have an important role in the classification process‚Äù (@rachmawanto).  In their study, Random Forest executed with the Abel Vika dataset achieves 100% accuracy, 100% precision, and 100% recall.  In contrast when the Pima Indian Diabetes Dataset is used with Random Forest, the results show at most scores of 75-85% for accuracy, precision, and recall.  The conclusion is that Random Forest excels when the data is noisy and unbalanced. 

Machine learning tools such as decision trees and random forests have recently become a tool used to predict risk factors of type 2 diabetes mellitus. With these tools they can potentially provide data/ results as a supplement in screening for diseases such as type 2 diabetes mellitus. Random forest allows for the generation of multiple classification trees by selecting subsets within a dataset and selecting subsets of predictor variables randomly, which then will aggregate the results of all the tested models which provides a random forest (@esmaily2018comparison). 

Random forest and decision trees both provide important data when used for analyzing data such as the output of the variable importance. The output of variable importance is used to measure the degree of association between given variables and the classification result (@esmaily2018comparison). This provides four measures of variable importance which helps to create good prediction models. Random forest models can provide good prediction models due to efficacy, sensitivity, and specificity (@esmaily2018comparison). 

Utilizing random forest provides many strengths when analyzing data. The strengths are that it doesn‚Äôt overfit, is robust to noise, has an internal mechanism called out-of-bag error, provides indices of variable importance, works with mixtures of both continuous and categorical variables, and can be used for cluster analysis and data imputation (@casanova2014application). During the process of creating random forest data two types of randomness will be used. The first is when a ‚Äútree‚Äù is grown using a bootstrapped version of the training data and the second is added during the growing stage of a ‚Äútree‚Äù by selecting a random sample of predictors (@casanova2014application).

Random forest is becoming a widely used tool in ecological and population genetics due to the efficiency of being able to analyze thousands of loci and account for nonadditive interactions (@brieuc2018practical). Using this data tool is allowing the study of large genomic data sets to be able to be analyzed as a promising method. Random forest usage has increased over the years for genetic analyses in evolutionary studies (@brieuc2018practical). This tool allows for flexibility in the data types when searching for response and predictor variables. Almost all types of data can be used; however, random forest does not allow missing values for response or predictor variables (@brieuc2018practical).

Some studies use random forest as a tool to compare it to traditional and other reliable machine learning techniques. Random forest was determined to be a clear beneficial tool to use compared to stratification during a statistical comparison of the two approaches (@mascaro2014tale). If random forest is implemented carefully it can be a very useful tool when spatial modeling.


## Methods



## Analysis and Results

### Data and Visualization
The Random Forest classification method is implemented in the Heart Failure Clinical Records dataset provided by the University of California Irvine Machine Learning Repository (Heart Failure Clinical Records. (2020). UCI Machine Learning Repository). The dataset comes from the Faisalabad Institute of Cardiology and the Allied Hospital in Faisalabad and consists of the medical records of 299 patients who experienced heart failure (Machine learning can predict survival of patients with heart failure from serum creatinine and ejection fraction alone | BMC Medical Informatics and Decision Making | Full Text (biomedcentral.com)). The features are as follows:

```{r}
HF <- data.frame("Feature Name" = c("age","anemia","creatine_phosphokinase","diabetes","ejection_fraction","high_blood_pressure", "platelets","serum_creatinine", "serum_sodium", "sex","smoking","time", "death_event"),
                 "Type" = c("integer","binary", "integer","binary","integer", "binary", "continuous","continuous","integer", "binary","binary","integer","binary"),
                 "Description" = c("age of patient", "decrease of red blood cells or hemoglobin", "level of the CPK enzyme in the blood", "if the patient has diabetes", "percentage of blood leaving the heart at each contraction", "if the patient has hypertension", "platelets in the blood","level of serum creatinine in the blood", "level of serum sodium in the blood", "woman or man", "if the patient smokes or not", "follow-up period in days", "if the patient died during the follow-up period" ))
HF
```

Table source: Heart Failure Clinical Records - UCI Machine Learning Repository
 
The goal is to create and evaluate a model to be used for predicting if a heart failure event may occur given features of a subject.

### Statistical Modeling

### Conclusion


## References
