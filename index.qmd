---
title: "Random Forest Analysis of Heart Failure Dataset"
author: "Sierra Landacre, Tim Leschke, Pamela Mishaw, Pallak Singh "
date: '`r Sys.Date()`'
format:
  html:
    code-fold: true
course: STA 6257 - Advanced Statistical Modeling
bibliography: references.bib # file contains bibtex for references
#always_allow_html: true # this allows to get PDF with HTML features
self-contained: true
execute: 
  warning: false
  message: false
editor: 
  markdown: 
    wrap: 72
---

## Introduction
Correctly classifying a person for being at risk for heart failure is essential for strong preventive health care.  Contributing factors are known to include coronary artery disease, hypertension, obesity, infection, certain medications, and lifestyle choices such as alcohol consumption, smoking, and drug abuse.  Understanding the influences of these competing factors is made easier through unsupervised machine learning which can find patterns and insights in data without explicit human guidance or instruction.  Random Forest is a promising machine learning model because it correctly classifies data from large data sets, is resistant to outliers, and is easy to use.  In this report we validate the application of Random Forest machine learning to support the accurate classification of heart failure among 299 patient profiles.

At the heart of the Random Forest model is the decision tree. A decision tree “is a unidirectional, compact, tree-like data structure where each branch point is an attribute-data test that partitions the data rows into two or more branches.” [@sheppard]. An optimized tree is shallow and has few branches. A decision tree may be used to perform classification or regression analysis to predict a future value given particular features of a subject.

Yuan et. al. state the most influential decision tree algorithm is Iterative Dichotomiser 3 (ID3) [@yuan]. ID3 is presented by Quinlan in his 1986 seminal paper <em>Induction of Decision Trees</em> [@quinlan]. This paper lays the foundation for using machine learning to address the bottleneck caused by expert systems. Quinlan demonstrates it is possible to use decision trees to generate knowledge and solve difficult problems through automation without much computation [@quinlan]. Low-levels of noise do not cause the tree building to “fall over a cliff” [@quinlan]. Furthermore, training the decision tree with noise has been discovered to yield optimal results.  In addition, unbalanced trees tend to perform statistically better than those balanced through stratification [@mascaro2014tale].

A decision tree does not have domain knowledge [@sheppard]. It begins as a data structure with a single node that divides the dataset into subsets according to a partitioning algorithm that learns from exposure to training data. Training data is a smaller representation of the main data set, and it contains examples of each class of datapoint found in the main data set. As the decision tree processes the training data, it grows branches and additional nodes to support the recursive partitioning of subsets into smaller subsets until this process terminates with each datapoint being assigned to a category found at each leaf node. This automated machine learning algorithm is challenged by poor-partitioning, noise, and long compute-time [@sheppard].

Poor-partitioning results in elongated decision trees with impure subsets.  An impure subset occurs when a branch point does not perform a pure classification.  Dirty subsets lead to incorrect classifications, and ultimately to poor heart failure classification in a preventive healthcare workflow.

Noise is another challenge facing machine learning with decision trees. Noise is any impurity found within the dataset. It is random, unpredictable, and unexpected data that disrupts the ability to correctly partition data into categories. It is what some call entropy [@sheppard]. The more entropy there is, the more challenging it is to correctly classify each datapoint. 

Long compute-time is influenced by both size and complexity of the classification dataset. Complex datasets with more categories require a decision tree with more nodes and more branches. Taller and fatter decision trees require more compute-time for processing. Pushing more data through a decision tree has a multiplying effect on the compute-time.

Leo Breiman addresses these three issues with his proposal of the Random Forest model
[@breiman]. Random Forest is a group of decision trees (a forest) that are created from identically distributed, independent random samples of data drawn with replacement from the original dataset [@breiman]. Breiman calls this random sample process a <em>bootstrap</em>, which others have re-worded as <em>bootstrapping</em>. At each node, a random subset of variables is taken from the array of features and the one that provides the best split is selected for that point. This randomness provides robustness in the presence of misclassifications and noise [@breiman], [@moradi2024random], [@casanova2014application]. This classification by aggregation is known as ensemble learning and is a proven approach for dealing with bad data [@sheppard].  Bad data, however, cannot include missing values for response or predictor variables [@brieuc2018practical]. 

Because each decision tree is processed with a different random set of data, each grows into a unique structure with a unique classification result. The final classification from Random Forest is a result of aggregating the results from each decision tree; a process Breiman calls <em>bagging</em> [@breiman]. This bagging technique addresses both the noise and poor-partitioning of traditional decision trees that results in inaccurate classification. By using multiple trees and aggregating the results, the effects of noise and poor-partitioning are minimized, and accuracy is increased [@breiman]. Furthermore, bagging can be used to calculate an “ongoing estimate of the generalization error” which is the error rate of the training set [@breiman]. This can be used to predict how well the model is classifying data.

Accuracy is optimized through a process called <em>boosting</em>. Boosting is a technique by which
more weight is given to the votes that come from decision trees that predict correctly versus those that do not predict correctly. Sheppard states “boosting earned a 1-2 percent overall improvement in the ability to predict the correct outcome while also achieving the percentage more consistently” [@sheppard]. Accuracy is also improved through a heuristic known as Ant Colony Optimization (ACO) [@boryczka]. This approach is inspired by ant colony behavior in which individual ants leave traces of pheromone to communicate to other ants regarding the best path to take. Similarly, the ACO algorithm improves dataset splitting by providing feedback to subsequent splitting choices. By improving the choice of criteria to split on, one achieves “maximum homogeneity in the decision classes” which improves overall decision tree accuracy [@boryczka].

An important feature of random Forest is its ability to provide measures of how strong a given variable is associated with a classification result.  These measures include <em>raw importance score for class 0</em>, <em>raw importance score for class 1</em>, <em>decrease in accuracy>, and the <em>Gini Index</em> [@esmaily2018comparison].  Gini Index is explained later.  These scores help users fine-tune the Random Forest method so optimal classification results are achieved.

In one study, researchers created a Random Forest model to predict in-hospital mortality for acute kidney injury patients in the intensive care unit [@lin2019predicting]. The Random Forest model displays the lowest Brier score (associated with accuracy) and the largest AUROC (associated with discrimination), meaning Random Forest produces the best values for these assessments of the compaired models, with the second best F1 score and accuracy. 

In another experiment, researchers developed a random forest regression model capable of accurately predicting future estimated glomerular filtration rates using electronic medical record data [@zhao2019predicting]. This tool allows identification of chronic kidney disease in its early stages to more likely prevent progression into end-stage renal disease. The random forest models that are created are found to have accuracies of about 90% for stages 2 and 3, and about 80% for stages 4 and 5 of chronic kidney disease. 

Scientists also investigate the accuracy of the Random Forest algorithm for the classification of gait as displaying or lacking the characteristics of hemiplegia, which is the paralysis of one side of the body [@luo2020random].  Researchers achieve a classification accuracy of 95.45%, which they state "is superior than all existing methods."  Because of the natural tendency for Random Forest to be resistant to over-fitting, the researchers "do not have to have careful control over the percentage of patients in the training data" [@luo2020random]. 

When the Random Forest model is applied to the classification of persons with type 1 and type 2 diabetes, the conclusion is that “the types of attributes used have an important role in the classification process” [@rachmawanto]. In their study, Random Forest executed with the Abel Vika dataset achieves 100% accuracy, 100% precision, and 100% recall. In contrast, when the Pima Indian Diabetes Dataset is used, the results show at most scores of 75-85% for accuracy, precision, and recall. The conclusion is that Random Forest excels when the data is noisy and unbalanced as is the case with the first dataset.

In practical applications, Random Forest’s scalability and parallelization capabilities are
noteworthy, allowing it to efficiently process large datasets using parallel computing techniques like MapReduce [@yuan]. This scalability makes random forest suitable for tasks ranging from clinical prediction in healthcare to large-scale genetic analyses, where handling big data is essential for achieving accurate and reliable results.

Random forest provides valuable insights into feature importance, aiding researchers in
understanding which variables contribute most significantly to predictions. This attribute is critical in applications where identifying key factors, such as genetic markers in genome-wide association studies, environmental variables in ecological analyses, and heart failure classification is paramount. Since the Random Forest model shows robustness in the face of noisy datasets, it is an ideal algorithm for studies that involve numerous features such as those found in the biomedical sciences. Its methodology is also more easily conveyed and understood by medical professionals than many other machine learning models, allowing for greater promise in successful real-world implementation. However, in other study types Random Forest is used due to simplicity and diversity, making it one of the commonly used algorithms used for solar energy predictions [@wang2020taxonomy].

In the following sections we explain the Methods pertaining to Random Forest.  This includes the algorithm, measuring node purity with the <em>Gini Index</em>, and aggregation of classification votes from individual decision trees via a processes called <em>bagging</em>.  The Analysis and Results section presents the classification of the Heart Failure Clinical Records dataset processed with R-code and explains how to tune the algorithm for optimized results [@misc_heart_failure_clinical_records_519].  Lastly, the Conclusion ties all of this work together and crowns Random Forest as a promising machine learning model for classifying people at risk for heart failure.  

## Methods

We assume the reader knows single classification trees.  A Random Forest performs classification via aggregated votes from a group of classification trees built from random data samples.

The algorithm for classification with Random Forest is provided below.   
(Copied, with additions, from [@hastie]).

<pre><span style="text-decoration:overline underline">Random Forest Algorithm                                                                               </span></pre>

<ol type ="1">
<li>For b = 1 to B where B is the number of trees in the forest:
    <ol type ="a">
      <li>Draw a bootstrap sample <b>Z*</b> of size <em>N</em> from the training data.</li>
      <li>Grow a random-forest tree <em>T<sub>b</sub></em> to the bootstrapped data, by recursively repeating the following steps for each terminal node of the tree, until the minimum node size <em>n<sub>min</sub></em> is reached.</li>
        <ol type = "i">
          <li>Select <em>m</em> variables at random from the <em>p</em> variables.</li>
          <li>Pick the best variable/split-point among the <em>m</em>.</li>
          <li>Split the node into two daughter nodes.</li>
        </ol>
      </li>
    </ol>
<li>Output the ensemble of trees.</li>

</ol>

To make a classification prediction at a new point <em>x</em>:

Let <em>C<span>&#770;</span><sub>b</sub>(<em>x</em>)</em> be the class prediction of the <em>b</em>th random-forest tree.  Then <em>C<span>&#770;</span><sub>rf</sub><sup>B</sup>(<em>x</em>) = <em> majority vote</em> {C<span>&#770;</span><sub>b</sub>(<em>x</em>)}<sub>1</sub><sup>B</sup></em>.

<pre><span style="text-decoration:overline">                                                                                                     </span></pre>

Figure 1 shows the generation of a Random Forest.  The uniqueness of each tree, as a result of random data samples, allows the ensemble to avoid misclassification, and improve classification accuracy.

![](Forest.png)
<center>Figure 1. (Modified from TikZ.net,  https://tikz.net/random-forest/, accessed 7/13/2024)</center>\    

The <em>Gini Index</em> is referred to as a measure of node purity [@james].  It can also be used to measure the importance of each predictor.  The Gini Index is defined by the following formula where K is the number of classes and ${\hat{p}_{mk}}$ is the proportion of observations in the <em>m</em>th region that are from the <em>k</em>th class.  A Gini Index of 0 represents perfect purity. 

$$D=-\sum_{n=1}^{K} {\hat{p}_{mk}}(1-\hat{p}_{mk})$$

<em>Bagging</em> is the aggregation of the results from each decision tree.  It is defined by the following formula where B is the number of training sets and $\hat{f}^{*b}$ is the prediction model.  Although bagging improves prediction accuracy, it makes interpreting the results harder as they cannot be visualized as easily as a single decision tree [@james].

$${\hat{f}bag(x) = 1/B \sum_{b=1}^{B}\hat{f}^{*b}(x)}$$


## Analysis and Results

### Data
The Random Forest classification method is implemented with the Heart Failure Clinical Records dataset provided by the University of California Irvine Machine Learning Repository [@misc_heart_failure_clinical_records_519]. The dataset comes from the Faisalabad Institute of Cardiology and the Allied Hospital in Faisalabad and consists of the medical records of 299 patients who experienced heart failure [@chicco2020machine].  

The features of the dataset are described in the following table:


```{r, eval = FALSE, echo=FALSE}
HF <- data.frame("Feature Name" = c("age","anemia","creatine_phosphokinase","diabetes","ejection_fraction","high_blood_pressure", "platelets","serum_creatinine", "serum_sodium", "sex","smoking","time", "death_event"),
                 "Type" = c("integer","binary", "integer","binary","integer", "binary", "continuous","continuous","integer", "binary","binary","integer","binary"),
                 "Description" = c("age of patient", "decrease of red blood cells or hemoglobin", "level of the CPK enzyme in the blood", "if the patient has diabetes", "percentage of blood leaving the heart at each contraction", "if the patient has hypertension", "platelets in the blood","level of serum creatinine in the blood", "level of serum sodium in the blood", "woman or man", "if the patient smokes or not", "follow-up period in days", "if the patient died during the follow-up period" ))
HF
```
<center>Heart Failure Clinical Records - UCI Machine Learning Repository</center>
 <style>
  table.tb { border-collapse: collapse; width:900px; }
  .tb th, .tb td { padding: 5px; border: solid 1px #777; }
  .tb th { background-color: lightblue;}
</style>

<table class="tb" title="Heart Failure Clinical Records - UCI Machine Learning Repository">
  <tr>
    <th>Feature Name</th>
    <th>Description</th>
  </tr>
  <tr>
    <td>Age</td>
    <td>Age of patient</td>
  </tr>
  <tr>
    <td>Anemia</td>
    <td>Decrease of red blood cell or hemoglobin</td>
  </tr>
  <tr>
    <td>Creatine Phosphokinase</td>
    <td>Level of CPK enzyme in the blood</td>
  </tr>
  <tr>
    <td>Diabetes</td>
    <td>If the patient has diabetes</td>
  </tr>
  <tr>
    <td>Ejection Fraction</td>
    <td>Percentage of blood leaving the heart at each contraction</td>
  </tr>
  <tr>
    <td>High Blood Pressure</td>
    <td>If the patient has hypertension</td>
  </tr>
  <tr>
    <td>Platelets</td>
    <td>Platelets in the blood</td>
  </tr>
  <tr>
    <td>Serum Creatinine</td>
    <td>Level of serum creatine in the blood</td>
  </tr>
  <tr>
    <td>Serum Sodium</td>
    <td>Level of serum sodium in the blood</td>
  <tr>
    <td>Sex</td>
    <td>Woman or Man</td>
  </tr>
  <tr>
    <td>Smoking</td>
    <td>If the patient smokes or not</td>
  <tr>
    <td>Time</td>
    <td>Follow-up period in days</td>
  </tr>
  <tr>
    <td>Death Event</td>
    <td>If the patient died during the follow-up period</td>
  </tr>
</table>
Source: [@misc_heart_failure_clinical_records_519]
 
The goal is to create and evaluate a model to be used for predicting if a heart failure event may occur given features of a subject.

### Install and Load Packages

```{r, eval = FALSE}
install.packages("caTools", repos="http://cran.us.r-project.org")
install.packages("randomForest", repos="http://cran.us.r-project.org")
install.packages("tidyverse", repos="http://cran.us.r-project.org")
install.packages("dplyr", repos="http://cran.us.r-project.org")
install.packages("party", repos="http://cran.us.r-project.org")

library(caTools)
library(randomForest)
library(tidyverse)
library(readr)
library(dplyr)
library(party)
library(caret)
```

### Loading the Dataset

{r, eval = FALSE}
#{r, eval = FALSE}
heart_df <- read_csv("heart_failure_clinical_records_dataset.csv")
head(heart_df, 5)

```

Note: A "positive" (1) in the Sex feature equates to male sex while "negative" (0) designates female. "DEATH_EVENT" is the target variable, signifying whether or not the subject has died.

### Examining the Data and Preprocessing

The dataset was examined for any null values.

{r, eval = FALSE}
#{r, eval = FALSE}
heart_df %>% summarise(across(everything(), ~ sum(is.na(.))))
```

No null values were found. The number of positive events (instances of heart failure deaths) versus negative events is produced to see if there is a significant imbalance.

{r, eval = FALSE}
#{r, eval = FALSE}
heart_df %>% count(DEATH_EVENT)
```

The number of negative events are about double that of the positive events. The Random Forest algorithm's robustness allows it to tolerate similar imbalances in the data.

The DEATH_EVENT feature is made a factor since it is a binary dependent variable.

{r, eval = FALSE}
#{r, eval = FALSE}
heart_df$DEATH_EVENT = as.factor(heart_df$DEATH_EVENT)
```

### Running the Random Forest Model

The dataset is split into training (80% of the dataset) and testing (20% of the data) sets. An id column was created to assist in the split.

{r, eval = FALSE}
#{r, eval = FALSE}
# Split the data, 80% goest to training and 20% to testing

set.seed(1)

samp <- sample(nrow(heart_df), 0.8 * nrow(heart_df))

heart_train <- heart_df[samp, ]

heart_test <- heart_df[-samp, ]

head(heart_train)
head(heart_test)
```

The training and testing sets are inspected.

{r, eval = FALSE}
#{r, eval = FALSE}
dim(heart_train)

dim(heart_test)
```

239 subjects were randomly placed into the training set and the remaining 50 were designated for testing.

The random forest model is then run on the dataset using the default settings.

{r, eval = FALSE}
#{r, eval = FALSE}
model <- randomForest(
  formula = DEATH_EVENT~.,
  data = heart_train,
  importance = TRUE
)

print(model)
```

The default random forest model produced an Out-of-the-Bag (OOB) estimate of error rate of 17.99%.

The confusion matrix shows the values of True Negatives, False Negatives, False Positives, and True Positives. There is a higher class error rate in the "positive" (death event occurrence) class than the "negative" (no death occurrence) class, with 36.0 % versus 9.76% rate, respectively.

{r, eval = FALSE}
#{r, eval = FALSE}
prediction <- predict(model, newdata = heart_test)

table(prediction, heart_test$DEATH_EVENT)
```

{r, eval = FALSE}
#{r, eval = FALSE}
# List the predicted classes produced by the model
prediction
```

{r, eval = FALSE}
#{r, eval = FALSE}
# Create a list of the prediction results alongside the actual DEATH_EVENT values from the test set
results <- cbind(prediction, heart_test$DEATH_EVENT)
colnames(results)<-c('pred','real')

results<-as.data.frame(results)

library(DT)
datatable(results)
```

{r, eval = FALSE}
#{r, eval = FALSE}
# Calculate the classification accuracy of the model
sum(prediction==heart_test$DEATH_EVENT) / nrow(heart_test)
```

{r, eval = FALSE}
#{r, eval = FALSE}
prediction <- predict(model, heart_test, type = "prob")
prediction
library(pROC)
ROC_rf <- roc(heart_test$DEATH_EVENT, prediction[,2])

# Area Under Curve (AUC) for each ROC curve (higher -> better)
ROC_rf_auc <- auc(ROC_rf)
```

{r, eval = FALSE}
#{r, eval = FALSE}
# plot ROC curves
plot(ROC_rf, col = "green", main = "ROC For Random Forest, Default Settings")

# print the performance of each model
paste("Accuracy % of default random forest: ", mean(heart_test$DEATH_EVENT == round(prediction[,2], digits = 0)))
paste("Area under curve of default random forest: ", ROC_rf_auc)
```

The classification accuracy with the default hyperparameters for the randomForest package comes to 88.3%.

{r, eval = FALSE}
#{r, eval = FALSE}
# Alternative way to create RF model with default parameters
control <- trainControl(method="repeatedcv", number=10, repeats=3)
seed <- 1
metric <- "Accuracy"
set.seed(seed)
mtry <- sqrt(ncol(heart_train))
tunegrid <- expand.grid(.mtry=mtry)
rf_default <- train(DEATH_EVENT~., data=heart_train, method="rf", metric=metric, tuneGrid=tunegrid, trControl=control)
print(rf_default)
```

This alternative implementation of the random forest algorithm produced a classification accuracy of 83.5%.

### Tuning the Model

{r, eval = FALSE}
#{r, eval = FALSE}
# Implement a random search for tuned mtry
control <- trainControl(method="repeatedcv", number=10, repeats=3, search="random")
set.seed(seed)
mtry <- sqrt(ncol(heart_train))
rf_random <- train(DEATH_EVENT~., data=heart_train, method="rf", metric=metric, tuneLength=13, trControl=control)
print(rf_random)
plot(rf_random)
```

The random search found the model's accuracy to be highest at mtry = 3. This was the default value for our randomForest model.

{r, eval = FALSE}
#{r, eval = FALSE}
# Use gridsearch to determine best mtry value
control <- trainControl(method="repeatedcv", number=10, repeats=3, search="grid")
set.seed(seed)
tunegrid <- expand.grid(.mtry=c(1:13))
rf_gridsearch <- train(DEATH_EVENT~., data=heart_train, method="rf", metric=metric, tuneGrid=tunegrid, trControl=control)
print(rf_gridsearch)
plot(rf_gridsearch)
```

The grid search suggests that the optimal mtry value is 4. The randomForest model that was created earlier can now be tested with this new mtry.

{r, eval = FALSE}
#{r, eval = FALSE}
model2 <- randomForest(
  formula = DEATH_EVENT~.,
  data = heart_train,
  mtry = 4,
  importance = TRUE
)

print(model2)
```

After changing the mtry hyperparameter to 4, the OOB estimate of error rate decreased slightly to 17.15%. The "positive" classification error did not improve– it worsened slightly, coming to about 30.7%. The "negative" classification error also increased slightly at a new value of about 11.0%.

Next, the number of trees (ntree) is tested over the range 500 (default) to 1500 to see if accuracy can be improved by changes in this hyperparameter.

{r, eval = FALSE}
#{r, eval = FALSE}
x <- heart_df[,1:12]
y <- heart_df[,13]

control <- trainControl(method = 'repeatedcv',
                        number = 10,
                        repeats = 3,
                        search = 'grid')
#create tunegrid
tunegrid <- expand.grid(.mtry = c(sqrt(ncol(heart_train))))
modellist <- list()

#train with different ntree parameters
for (ntree in seq(500,1500,100)){
  set.seed(1)
  fit <- train(DEATH_EVENT~.,
               data = heart_train,
               method = 'rf',
               metric = 'Accuracy',
               tuneGrid = tunegrid,
               trControl = control,
               ntree = ntree)
  key <- toString(ntree)
  modellist[[key]] <- fit
}

#Compare results
results <- resamples(modellist)
summary(results)
```

```{r]
#{r, eval = FALSE}
dotplot(results)
```

The results suggest that the optimal ntree value is 600. This new ntree values is tested, along with the tuned mtry value of 4, using the randomForest model from before.

{r, eval = FALSE}
#{r, eval = FALSE}
model3 <- randomForest(
  formula = DEATH_EVENT~.,
  data = heart_train,
  mtry = 4,
  ntree = 600,
  importance = TRUE
)

print(model3)
```

The OOB value slightly decreased to 16.32% while the classification error rates returned to those generated by the default settings for randomForest.

{r, eval = FALSE}
#{r, eval = FALSE}
prediction2 <- predict(model3, newdata = heart_test)

table(prediction2, heart_test$DEATH_EVENT)
```

{r, eval = FALSE}
#{r, eval = FALSE}
# Create a list of the prediction results alongside the actual DEATH_EVENT values from the test set 
results2 <- cbind(prediction2, heart_test$DEATH_EVENT) 
colnames(results2)<-c('pred','real')  
results2<-as.data.frame(results2)  
datatable(results2)
```

{r, eval = FALSE}
#{r, eval = FALSE}
# Calculate the classification accuracy of the model 
sum(prediction2==heart_test$DEATH_EVENT) / nrow(heart_test)
```

{r, eval = FALSE}
#{r, eval = FALSE}
prediction2 <- predict(model3, heart_test, type = "prob") 
prediction2 
ROC_rf2 <- roc(heart_test$DEATH_EVENT, prediction2[,2])  
# Area Under Curve (AUC) for each ROC curve (higher -> better) 
ROC_rf2_auc <- auc(ROC_rf2)
```

{r, eval = FALSE}
#{r, eval = FALSE}
# plot ROC curves 
plot(ROC_rf2, col = "red", main = "ROC For Random Forest, Default Settings")
lines(ROC_rf, col = "green")
# print the performance of each model 
paste("Accuracy % of default random forest: ", mean(heart_test$DEATH_EVENT == round(prediction[,2], digits = 0)))
paste("Area under curve of default random forest: ", ROC_rf_auc)

paste("Accuracy % of tuned random forest: ", mean(heart_test$DEATH_EVENT == round(prediction2[,2], digits = 0))) 
paste("Area under curve of tuned random forest: ", ROC_rf2_auc)
```

The accuracy and area under the receiver operating characteristic curve (AUC-ROC) are higher for the model produced using the default hyperparameters (88.3% and 0.932, respectively) compared to the "tuned" model (86.7% and 0.926, respectively).

The output of a Random Forest model is influenced by subsample size, the number of variables assessed at each node, the maximum nodesize of each leaf, and the number of predictors generated [@scornet2017tuning]. In R, the defaults are a subsample size of sqrt(𝑑) for classification (where d is the total number of dimensions), the generation of 500 predictors, and a nodesize of 5 [@scornet2017tuning]. In one comparison study, the balanced accuracy was found to be optimal at 1300 trees and 3 nodes in R and led to an increase of 0.09 (9%) in balanced accuracy as well as a 0.14 (14%) in the AUC-ROC compared to the values generated by the default hyperparameters [@lotsch2022biomedical].The results suggest the importance of tuning the hyperparameters of random forest algorithms, particularly in the case of exploratory analysis that has initial unfavorable accuracy. In the case of this dataset, the model with the default hyperparameters actually provided better results than the tuned model which implies that tuning is not always necessary or helpful for improving performance for the random forest algorithm.

### Statistical Modeling

## Conclusion


## References
