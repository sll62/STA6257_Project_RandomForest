---
title: "Random Forest - Data Science Capstone"
authors: "Sierra Landacre, Pamela Mishaw, Pallak Singh, and Tim Leschke"
date: '`r Sys.Date()`'
format:
  html:
    code-fold: true
course: STA 6257 - Advanced Statistical Modeling
bibliography: references.bib # file contains bibtex for references
#always_allow_html: true # this allows to get PDF with HTML features
self-contained: true
execute: 
  warning: false
  message: false
editor: 
  markdown: 
    wrap: 72
---

## Introduction

### What is "method"?

This is an introduction to LASSO regression, which is a non-parametric
estimator that estimates the conditional expectation of two variables
which is random. The goal of a kernel regression is to discover the
non-linear relationship between two random variables. To discover the
non-linear relationship, kernel estimator or kernel smoothing is the
main method to estimate the curve for non-parametric statistics. In
kernel estimator, weight function is known as kernel function
[@efr2008]. Cite this paper [@bro2014principal]. The GEE [@wang2014].
The PCA [@daffertshofer2004pca]

This is my work and I want to add more work...

### Related work

This section is going to cover the literature review...

The organization of this section is inspired by Clinton Sheppard [@sheppard].

#### Decision Tree Algorithms

* Decision tree algorithms that perform classification with ONE TEST to determine the partition for NON-CONTINUOUS/NON-NUMERICAL data.

* Decision tree algorithms that perform classification with MULTIPLE TESTS to determine the partition for NON-CONTINUOUS/NON-NUMERICAL data.

* Decision tree algorithms that handle NUMERICAL (but non-continuous) data.

* Decision tree algorithms that use pruning to improve the accuracy of predictions.

#### Random Forest

* Better at handling noisy data than a decision tree.

* Better compute time by only working with a subset (sample) of the data

#### Random Forest & Regression Analysis

* Decision tree/Random Forests algorithms that use REGRESSION ANALYSIS to predict continuous outcomes (ranges of numeric values, like age 60-75 years).

#### Random Forest & Boosting

* Decision tree/Random Forest algorithms that use BOOSTING to increase prediction accuracy.

## Methods

The common non-parametric regression model is
$Y_i = m(X_i) + \varepsilon_i$, where $Y_i$ can be defined as the sum of
the regression function value $m(x)$ for $X_i$. Here $m(x)$ is unknown
and $\varepsilon_i$ some errors. With the help of this definition, we
can create the estimation for local averaging i.e. $m(x)$ can be
estimated with the product of $Y_i$ average and $X_i$ is near to $x$. In
other words, this means that we are discovering the line through the
data points with the help of surrounding data points. The estimation
formula is printed below [@R-base]:

$$
M_n(x) = \sum_{i=1}^{n} W_n (X_i) Y_i  \tag{1}
$$ $W_n(x)$ is the sum of weights that belongs to all real numbers.
Weights are positive numbers and small if $X_i$ is far from $x$.

Another equation:

$$
y_i = \beta_0 + \beta_1 X_1 +\varepsilon_i
$$

## Analysis and Results

### Data and Visualization

A study was conducted to determine how...


```{r, warning=FALSE, echo=T, message=FALSE}
# loading packages 
library(tidyverse)
library(knitr)
#library(ggthemes)
library(ggrepel)
library(dslabs)
```

```{r, warning=FALSE, echo=TRUE}
# Load Data
kable(head(murders))

ggplot1 = murders %>% ggplot(mapping = aes(x=population/10^6, y=total)) 

  ggplot1 + geom_point(aes(col=region), size = 4) +
  geom_text_repel(aes(label=abb)) +
  scale_x_log10() +
  scale_y_log10() +
  geom_smooth(formula = "y~x", method=lm,se = F)+
  xlab("Populations in millions (log10 scale)") + 
  ylab("Total number of murders (log10 scale)") +
  ggtitle("US Gun Murders in 2010") +
  scale_color_discrete(name = "Region")+
      theme_bw()
  

```

### Statistical Modeling

```{r}

```

### Conclusion

### Random Forest Created by Tim

```{r}
wine <- read.csv(url("https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv"), header = TRUE, sep = ";") # This command is used to load the dataset

head(wine) # Display the head and dimensions of wine dataset

dim(wine)

barplot(table(wine$quality)) # Barplot to see the quality of wines. The output looks like below
```

```{r}
# Now, we have to convert the quality values into factors

wine$taste <- ifelse(wine$quality < 5, "bad", "good")

wine$taste[wine$quality == 5] <- "normal"

wine$taste[wine$quality == 6] <- "normal"

wine$taste <- as.factor(wine$taste)

str(wine$taste)

barplot(table(wine$taste)) # Barplot to view the taste of wines. The output is shown below.

table(wine$taste) 
```

```{r}
# Next, we need to split the data into training and testing. 80% for training, 20% for testing.


set.seed(123)

samp <- sample(nrow(wine), 0.8 * nrow(wine))

train <- wine[samp, ]

test <- wine[-samp, ]
```

```{r}
# Moving onto the Data visualization

library(ggplot2)


ggplot(wine,aes(fixed.acidity,volatile.acidity))+ geom_point(aes(color=taste))# This command is used to display a scatter plot. The output looks like below

```

```{r}
ggplot(wine,aes(alcohol)) + geom_histogram(aes(fill=taste),color='black',bins=50) # This command is used to display a stacked bar chart. The output looks like below

```

```{r}
dim(train)

dim(test)  # Checks the dimensions of training and testing dataset


install.packages('randomForest', repo="https://cran.r-project.org/package=randomForest")

library(randomForest)           # Install the random forest library


# Now that we have installed the randomforest library, let’s build the random forest model


model <- randomForest(taste ~ . - quality, data = train, ntree = 1000, mtry = 5)

model

model$confusion


# The next step is to validate our model using the test data

prediction <- predict(model, newdata = test)

table(prediction, test$taste)

prediction
```

```{r}
# Now, let’s display the predicted vs. the actual values


results<-cbind(prediction,test$taste)

results

colnames(results)<-c('pred','real')

results<-as.data.frame(results)

#View(results)
library(DT)
datatable(results)


# Finally, let’s calculate the accuracy of the model

sum(prediction==test$taste) / nrow(test) # The output is as shown below
```



## References
