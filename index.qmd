---
title: "Random Forest Analysis of Heart Failure Dataset"
author: "Sierra Landacre, Tim Leschke, Pamela Mishaw, Pallak Singh "
date: '`r Sys.Date()`'
format:
  html:
    code-fold: true
course: STA 6257 - Advanced Statistical Modeling
bibliography: references.bib # file contains bibtex for references
#always_allow_html: true # this allows to get PDF with HTML features
self-contained: true
execute: 
  warning: false
  message: false
editor: 
  markdown: 
    wrap: 72
---

## Introduction
Teaching computers to learn through experience began formally in 1959 when IBM computer gaming pioneer Arthur Samuel coined the term machine learning in support of what is now known as artificial intelligence.[i] Although the modeling of human cognitive function was discussed well before Samuel, he is credited with giving this discipline its identity. 

At the heart of the machine learning algorithm is the decision tree. A decision tree “is a unidirectional, compact, tree-like data structure where each branch point is an attribute-data test that partitions the data rows into two or more branches.”  (@sheppard). An optimized tree is shallow and has few branches.  A decision tree may be used to perform classification or regression analysis to predict a future value given particular features of a subject. Consider, for example, a dataset composed of items that are to be categorized as either land vehicles, watercraft, or aircraft. The first attribute-data test might evaluate if the item flies. If yes, the item is classified as an aircraft. If no, a second attribute-data test will evaluate if the item floats. If yes, the item is classified as a watercraft. If no, the item is classified as a land vehicle. 

A decision tree does not have domain knowledge (@sheppard). It begins as a data structure with a single node that divides the dataset into subsets according to a partitioning algorithm that learns from exposure to training data. Training data is a smaller representation of the main data set, and it contains examples of each class of datapoint found in the main dataset. As the decision tree processes the training data, it grows branches and additional nodes to support the recursive partitioning of subsets into smaller subset until this process terminates with each datapoint being assigned to a category found at each leaf node. This automated machine learning algorithm is challenged by poor-partitioning, noise, and long compute-time (@sheppard).

Noise is another challenge facing machine learning with decision trees. Noise is any impurity found within the dataset. It is random, unpredictable, and unexpected data that disrupts the ability to correctly partition data into categories. It is what some call entropy (@sheppard). The more entropy there is, the more challenging it is to correctly classify each datapoint.

Long compute-time is influenced by both size and complexity. Complex datasets with more categories require a decision tree with more nodes and more branches. Taller and fatter decision trees require more compute-time for processing. Pushing more data through a decision tree has a multiplying effect on the compute-time.
 
Leo Breiman addresses these three issues with his proposal of his Random Forest model (@breiman). His execution of the Random Forest method is a group of decision trees (a forest) that are created from identically distributed, independent random samples of data drawn with replacement from the original dataset (@breiman). Breiman calls this random sample process a bootstrap, which others have re-worded as bootstrapping (https://www.youtube.com/watch?v=v6VJ2RO66Ag). 
 
At each node, a random subset of variables is taken from the array of features and the one that provides the best split is selected for that point. This randomness provides robustness in the presence of misclassifications produced by the minority of inaccurate trees as these are compensated for by the votes of the majority of accurate trees (@breiman). This classification by aggregation is known as ensemble learning and is a proven approach for dealing with bad data (@sheppard).  
 
Because each decision tree is processed with a different random set of data, each grows into a unique structure with a unique classification result. The final classification from Random Forest is a result of aggregating the results from each decision tree; a process Breiman calls bagging (@breiman). This bagging technique addresses both the noise and poor-partitioning of traditional decision trees that results in inaccurate classification. However, by using multiple trees and aggregating the results, the effects of noise and poor-partitioning are minimized, and accuracy is increased (@breiman).  Furthermore, bagging can be used to calculate an “ongoing estimate of the generalization error” which is the error rate of the training set (@breiman).  This can be used to predict how well the model is performing.
 
Accuracy is optimized through a process called boosting.  Boosting is a technique by which more weight is given to the votes that come from decision trees that predict correctly versus those that do not predict correctly. Sheppard states “boosting earned a 1-2 percent overall improvement in the ability to predict the correct outcome while also achieving the percentage more consistently” (@sheppard).
 
Accuracy is also improved through a heuristic known as Ant Colony Optimization (ACO) (@boryczka).  This approach is inspired by ant colony behavior in which individual ants leave traces of pheromone to communicate to other ants regarding the best path to take.  Similarly, the ACO algorithm improves dataset splitting by providing feedback to future splitting choices.  By improving the choice of criteria to split on, one can achieve “maximum homogeneity in the decision classes” which improves overall decision tree accuracy (@boryczka). 
 
Yuan et. al. state the most influential decision tree algorithm is ID3 (@yuan).  ID3 is presented by Quinlan in his 1986 seminal paper Induction of Decision Trees (@quinlan).  This paper lays the foundation for using machine learning to address the bottleneck caused by expert systems.  Quinlan demonstrates it is possible to use decision trees to generate knowledge and solve difficult problems through automation without much computation (@quinlan).  Low-levels of noise do not cause the tree building to “fall over a cliff” (@quinlan).  Furthermore, it is good to train the decision tree with noise if the dataset has noise.
 
Throughput is optimized through parallelization.  Yuan et. al. perform Decision Tree classification with Big Data via a MapReduce algorithm (MapReduce – GA Optimization Decision Tree) that implements parallelization via cloud computing and Hadoop technology. The researchers claim a “higher classification accuracy and shorter running time compared with the traditional decision tree algorithm” (@yuan).
 
 
Since the Random Forest model shows robustness in the face of noisy datasets, it is an ideal algorithm for studies that involve numerous features such as those found in the biomedical sciences. Its methodology is also more easily conveyed and understood by medical professionals than many other machine learning models, allowing for greater promise in successful real-world implementation. 
 
In one such study, researchers create a random forest model to predict in-hospital mortality for acute kidney injury (AKI) patients in the intensive care unit (ICU) (@lin2019predicting). The Random Forest model displays the lowest Brier score (associated with accuracy) and the largest AUROC (associated with discrimination), meaning RF produces the best values for these assessments of the four models, with the second best F1 score and accuracy. The support vector machine (SVM) model had the best accuracy and F1 score but is not statistically significant in difference from the RF results. In another experiment, researchers develop a random forest regression model capable of accurately predicting future estimated glomerular filtration rates (eGFR) using electronic medical record (EMR) data (@zhao2019predicting). This tool allows identification of chronic kidney disease (CKD) in its early stages to more likely prevent progression into end-stage renal disease. The random forest models that are created are found to have accuracies of about 90% for stages 2 and 3 of the CKD and about 80% for stages 4 and 5. Scientists also investigate the accuracy of the Random Forest algorithm for the classification of gait as displaying or lacking the characteristics of hemiplegia, which is the paralysis of one side of the body (@luo2020random).
 
In another study, researchers apply Random Forest to the classification of persons with type 1 and type 2 diabetes.  The conclusion is that “the types of attributes used have an important role in the classification process” (@rachmawanto).  In their study, Random Forest executed with the Abel Vika dataset achieves 100% accuracy, 100% precision, and 100% recall.  In contrast when the Pima Indian Diabetes Dataset is used with Random Forest, the results show at most scores of 75-85% for accuracy, precision, and recall.  The conclusion is that Random Forest excels when the data is noisy and unbalanced. 

Machine learning tools such as decision trees and random forests have recently become a tool used to predict risk factors of type 2 diabetes mellitus. With these tools they can potentially provide data/ results as a supplement in screening for diseases such as type 2 diabetes mellitus. Random forest allows for the generation of multiple classification trees by selecting subsets within a dataset and selecting subsets of predictor variables randomly, which then will aggregate the results of all the tested models which provides a random forest (@esmaily2018comparison). 

Random forest and decision trees both provide important data when used for analyzing data such as the output of the variable importance. The output of variable importance is used to measure the degree of association between given variables and the classification result (@esmaily2018comparison). This provides four measures of variable importance which helps to create good prediction models. Random forest models can provide good prediction models due to efficacy, sensitivity, and specificity (@esmaily2018comparison). 

Utilizing random forest provides many strengths when analyzing data. The strengths are that it doesn’t overfit, is robust to noise, has an internal mechanism called out-of-bag error, provides indices of variable importance, works with mixtures of both continuous and categorical variables, and can be used for cluster analysis and data imputation (@casanova2014application). During the process of creating random forest data two types of randomness will be used. The first is when a “tree” is grown using a bootstrapped version of the training data and the second is added during the growing stage of a “tree” by selecting a random sample of predictors (@casanova2014application).

Random forest is becoming a widely used tool in ecological and population genetics due to the efficiency of being able to analyze thousands of loci and account for nonadditive interactions (@brieuc2018practical). Using this data tool is allowing the study of large genomic data sets to be able to be analyzed as a promising method. Random forest usage has increased over the years for genetic analyses in evolutionary studies (@brieuc2018practical). This tool allows for flexibility in the data types when searching for response and predictor variables. Almost all types of data can be used; however, random forest does not allow missing values for response or predictor variables (@brieuc2018practical).

Some studies use random forest as a tool to compare it to traditional and other reliable machine learning techniques. Random forest was determined to be a clear beneficial tool to use compared to stratification during a statistical comparison of the two approaches (@mascaro2014tale). If random forest is implemented carefully it can be a very useful tool when spatial modeling.

Mohammadamin Moradi's research, detailed in "Random forests for detecting weak signals and extracting physical information: A case study of magnetic navigation," highlights the robustness of random forests in handling noisy feature data and mitigating overfitting through ensemble learning(@moradi2024random). This approach involves training multiple decision trees on subsets of features and data points, aggregating their predictions to enhance accuracy in predicting target variables despite noise. In the context of magnetic navigation in GPS-denied environments, such as aircraft cockpits, random forests excel in detecting and utilizing Earth's anomaly magnetic field amidst interference from cockpit electronics and dominant Earth magnetic components.

The study underscores the independence of random forests from traditional calibration methods like the Tolles-Lawson model, showcasing their ability to predict aircraft positions accurately based solely on feature data from onboard sensors. This autonomous capability simplifies navigation tasks and ensures reliable performance in environments where distinguishing weak signals from noise is crucial for precise positioning. Lai's work demonstrates the versatility and effectiveness of random forests in real-time applications requiring robust signal processing, advancing the field of machine learning in aerospace navigation and beyond(@moradi2024random).

Random forest (RF) has emerged as a powerful machine learning algorithm renowned for its robustness and versatility across various domains. At its core, RF constructs multiple decision trees during training, each trained on a random subset of the data and using a random subset of features. This randomness helps mitigate overfitting and enhances the model's ability to generalize well to unseen data, making it particularly effective in handling noisy datasets commonly found in medical and ecological studies.

One of the key strengths of RF lies in its capability to handle both continuous and categorical data without requiring extensive preprocessing. This flexibility has made it a preferred choice in fields such as genetics and ecological modeling, where datasets often contain diverse types of variables and complex interactions.

Moreover, RF provides valuable insights into feature importance, aiding researchers in understanding which variables contribute most significantly to predictions. This attribute is critical in applications where identifying key factors, such as genetic markers in genome-wide association studies or environmental variables in ecological analyses, is paramount.

In practical applications, RF's scalability and parallelization capabilities are noteworthy, allowing it to efficiently process large datasets using parallel computing techniques like MapReduce. This scalability makes RF suitable for tasks ranging from clinical prediction in healthcare to large-scale genetic analyses, where handling big data is essential for achieving accurate and reliable results.



## Methods

We assume the reader knows single classification trees.  A Random Forest performs classification via aggregated votes from a group of classification trees built from random data samples.

The algorithm for classification with Random Forest is provided below.   
(Copied, with additions, from (@hastie)).

<pre><span style="text-decoration:overline underline">Random Forest Algorithm                                                                               </span></pre>

<ol type ="1">
<li>For b = 1 to B where B is the number of trees in the forest:
    <ol type ="a">
      <li>Draw a bootstrap sample <b>Z*</b> of size <em>N</em> from the training data.</li>
      <li>Grow a random-forest tree <em>T<sub>b</sub></em> to the bootstrapped data, by recursively repeating the following steps for each terminal node of the tree, until the minimum node size <em>n<sub>min</sub></em> is reached.</li>
        <ol type = "i">
          <li>Select <em>m</em> variables at random from the <em>p</em> variables.</li>
          <li>Pick the best variable/split-point among the <em>m</em>.</li>
          <li>Split the node into two daughter nodes.</li>
        </ol>
      </li>
    </ol>
<li>Output the ensemble of trees.</li>

</ol>

To make a classification prediction at a new point <em>x</em>:

Let <em>C<span>&#770;</span><sub>b</sub>(<em>x</em>)</em> be the class prediction of the <em>b</em>th random-forest tree.  Then <em>C<span>&#770;</span><sub>rf</sub><sup>B</sup>(<em>x</em>) = <em> majority vote</em> {C<span>&#770;</span><sub>b</sub>(<em>x</em>)}<sub>1</sub><sup>B</sup></em>.

<pre><span style="text-decoration:overline">                                                                                                     </span></pre>

Figure 1 shows the generation of a Random Forest.  The uniqueness of each tree, as a result of random data samples, allows the ensemble to avoid misclassification, and improve classification accuracy.

![](Forest.png)
<center>Figure 1. (Modified from TikZ.net,  https://tikz.net/random-forest/, accessed 7/13/2024)</center>\    

The <em>Gini Index</em> is referred to as a measure of node purity (@james).  It can also be used to measure the importance of each predictor.  The Gini Index is defined by the following formula where K is the number of classes and ${\hat{p}_{mk}}$ is the proportion of observations in the <em>m</em>th region that are from the <em>k</em>th class.  A Gini Index of 0 represents perfect purity. 

$$D=-\sum_{n=1}^{K} {\hat{p}_{mk}}(1-\hat{p}_{mk})$$

<em>Bagging</em> is the aggregation of the results from each decision tree.  It is defined by the following formula where B is the number of training sets and $\hat{f}^{*b}$ is the prediction model.  Although bagging improves prediction accuracy, it makes interpreting the results harder as they cannot be visualized as easily as a single decision tree (@james).

$${\hat{f}bag(x) = 1/B \sum_{b=1}^{B}\hat{f}^{*b}(x)}$$


## Analysis and Results

### Data and Visualization
The Random Forest classification method is implemented in the Heart Failure Clinical Records dataset provided by the University of California Irvine Machine Learning Repository [@misc_heart_failure_clinical_records_519]. The dataset comes from the Faisalabad Institute of Cardiology and the Allied Hospital in Faisalabad and consists of the medical records of 299 patients who experienced heart failure [@chicco2020machine]. The features are as follows:

```{r}
HF <- data.frame("Feature Name" = c("age","anemia","creatine_phosphokinase","diabetes","ejection_fraction","high_blood_pressure", "platelets","serum_creatinine", "serum_sodium", "sex","smoking","time", "death_event"),
                 "Type" = c("integer","binary", "integer","binary","integer", "binary", "continuous","continuous","integer", "binary","binary","integer","binary"),
                 "Description" = c("age of patient", "decrease of red blood cells or hemoglobin", "level of the CPK enzyme in the blood", "if the patient has diabetes", "percentage of blood leaving the heart at each contraction", "if the patient has hypertension", "platelets in the blood","level of serum creatinine in the blood", "level of serum sodium in the blood", "woman or man", "if the patient smokes or not", "follow-up period in days", "if the patient died during the follow-up period" ))
HF
```

Table source: Heart Failure Clinical Records - UCI Machine Learning Repository [@misc_heart_failure_clinical_records_519]
 
The goal is to create and evaluate a model to be used for predicting if a heart failure event may occur given features of a subject.

### Install and Load Packages

```{r}
install.packages("caTools")
install.packages("randomForest")
install.packages("tidyverse")
install.packages("dplyr")
install.packages("party")

library(caTools)
library(randomForest)
library(tidyverse)
library(readr)
library(dplyr)
library(party)
library(caret)
```

### Loading the Dataset

```{r}
heart_df <- read_csv("~/AdvancedStatisticalModeling/STA6257_Project_RandomForest/heart_failure_clinical_records_dataset.csv")
head(heart_df, 5)

```

Note: A "positive" (1) in the Sex feature equates to male sex while "negative" (0) designates female. "DEATH_EVENT" is the target variable, signifying whether or not the subject has died.

### Examining the Data and Preprocessing

The dataset was examined for any null values.

```{r}
heart_df %>% summarise(across(everything(), ~ sum(is.na(.))))
```

No null values were found. The number of positive events (instances of heart failure deaths) versus negative events is produced to see if there is a significant imbalance.

```{r}
heart_df %>% count(DEATH_EVENT)
```

The number of negative events are about double that of the positive events. The Random Forest algorithm's robustness allows it to tolerate similar imbalances in the data.

The DEATH_EVENT feature is made a factor since it is a binary dependent variable.

```{r}
heart_df$DEATH_EVENT = as.factor(heart_df$DEATH_EVENT)
```

### Running the Random Forest Model

The dataset is split into training (80% of the dataset) and testing (20% of the data) sets. An id column was created to assist in the split.

```{r}
# Split the data, 80% goest to training and 20% to testing

set.seed(1)

samp <- sample(nrow(heart_df), 0.8 * nrow(heart_df))

heart_train <- heart_df[samp, ]

heart_test <- heart_df[-samp, ]

head(heart_train)
head(heart_test)
```

The training and testing sets are inspected.

```{r}
dim(heart_train)

dim(heart_test)
```

239 subjects were randomly placed into the training set and the remaining 50 were designated for testing.

The random forest model is then run on the dataset using the default settings.

```{r}
model <- randomForest(
  formula = DEATH_EVENT~.,
  data = heart_train,
  importance = TRUE
)

print(model)
```

The default random forest model produced an Out-of-the-Bag (OOB) estimate of error rate of 17.99%.

The confusion matrix shows the values of True Negatives, False Negatives, False Positives, and True Positives. There is a higher class error rate in the "positive" (death event occurrence) class than the "negative" (no death occurrence) class, with 36.0 % versus 9.76% rate, respectively.

```{r}
prediction <- predict(model, newdata = heart_test)

table(prediction, heart_test$DEATH_EVENT)
```

```{r}
# List the predicted classes produced by the model
prediction
```

```{r}
# Create a list of the prediction results alongside the actual DEATH_EVENT values from the test set
results <- cbind(prediction, heart_test$DEATH_EVENT)
colnames(results)<-c('pred','real')

results<-as.data.frame(results)

library(DT)
datatable(results)
```

```{r}
# Calculate the classification accuracy of the model
sum(prediction==heart_test$DEATH_EVENT) / nrow(heart_test)
```

```{r}
prediction <- predict(model, heart_test, type = "prob")
prediction
library(pROC)
ROC_rf <- roc(heart_test$DEATH_EVENT, prediction[,2])

# Area Under Curve (AUC) for each ROC curve (higher -> better)
ROC_rf_auc <- auc(ROC_rf)
```

```{r}
# plot ROC curves
plot(ROC_rf, col = "green", main = "ROC For Random Forest, Default Settings")

# print the performance of each model
paste("Accuracy % of default random forest: ", mean(heart_test$DEATH_EVENT == round(prediction[,2], digits = 0)))
paste("Area under curve of default random forest: ", ROC_rf_auc)
```

The classification accuracy with the default hyperparameters for the randomForest package comes to 88.3%.

```{r}
# Alternative way to create RF model with default parameters
control <- trainControl(method="repeatedcv", number=10, repeats=3)
seed <- 1
metric <- "Accuracy"
set.seed(seed)
mtry <- sqrt(ncol(heart_train))
tunegrid <- expand.grid(.mtry=mtry)
rf_default <- train(DEATH_EVENT~., data=heart_train, method="rf", metric=metric, tuneGrid=tunegrid, trControl=control)
print(rf_default)
```

This alternative implementation of the random forest algorithm produced a classification accuracy of 83.5%.

### Tuning the Model

```{r}
# Implement a random search for tuned mtry
control <- trainControl(method="repeatedcv", number=10, repeats=3, search="random")
set.seed(seed)
mtry <- sqrt(ncol(heart_train))
rf_random <- train(DEATH_EVENT~., data=heart_train, method="rf", metric=metric, tuneLength=13, trControl=control)
print(rf_random)
plot(rf_random)
```

The random search found the model's accuracy to be highest at mtry = 3. This was the default value for our randomForest model.

```{r}
# Use gridsearch to determine best mtry value
control <- trainControl(method="repeatedcv", number=10, repeats=3, search="grid")
set.seed(seed)
tunegrid <- expand.grid(.mtry=c(1:13))
rf_gridsearch <- train(DEATH_EVENT~., data=heart_train, method="rf", metric=metric, tuneGrid=tunegrid, trControl=control)
print(rf_gridsearch)
plot(rf_gridsearch)
```

The grid search suggests that the optimal mtry value is 4. The randomForest model that was created earlier can now be tested with this new mtry.

```{r}
model2 <- randomForest(
  formula = DEATH_EVENT~.,
  data = heart_train,
  mtry = 4,
  importance = TRUE
)

print(model2)
```

After changing the mtry hyperparameter to 4, the OOB estimate of error rate decreased slightly to 17.15%. The "positive" classification error did not improve– it worsened slightly, coming to about 30.7%. The "negative" classification error also increased slightly at a new value of about 11.0%.

Next, the number of trees (ntree) is tested over the range 500 (default) to 1500 to see if accuracy can be improved by changes in this hyperparameter.

```{r}
x <- heart_df[,1:12]
y <- heart_df[,13]

control <- trainControl(method = 'repeatedcv',
                        number = 10,
                        repeats = 3,
                        search = 'grid')
#create tunegrid
tunegrid <- expand.grid(.mtry = c(sqrt(ncol(heart_train))))
modellist <- list()

#train with different ntree parameters
for (ntree in seq(500,1500,100)){
  set.seed(1)
  fit <- train(DEATH_EVENT~.,
               data = heart_train,
               method = 'rf',
               metric = 'Accuracy',
               tuneGrid = tunegrid,
               trControl = control,
               ntree = ntree)
  key <- toString(ntree)
  modellist[[key]] <- fit
}

#Compare results
results <- resamples(modellist)
summary(results)
```

```{r}
dotplot(results)
```

The results suggest that the optimal ntree value is 600. This new ntree values is tested, along with the tuned mtry value of 4, using the randomForest model from before.

```{r}
model3 <- randomForest(
  formula = DEATH_EVENT~.,
  data = heart_train,
  mtry = 4,
  ntree = 600,
  importance = TRUE
)

print(model3)
```

The OOB value slightly decreased to 16.32% while the classification error rates returned to those generated by the default settings for randomForest.

```{r}
prediction2 <- predict(model3, newdata = heart_test)

table(prediction2, heart_test$DEATH_EVENT)
```

```{r}
# Create a list of the prediction results alongside the actual DEATH_EVENT values from the test set 
results2 <- cbind(prediction2, heart_test$DEATH_EVENT) 
colnames(results2)<-c('pred','real')  
results2<-as.data.frame(results2)  
datatable(results2)
```

```{r}
# Calculate the classification accuracy of the model 
sum(prediction2==heart_test$DEATH_EVENT) / nrow(heart_test)
```

```{r}
prediction2 <- predict(model3, heart_test, type = "prob") 
prediction2 
ROC_rf2 <- roc(heart_test$DEATH_EVENT, prediction2[,2])  
# Area Under Curve (AUC) for each ROC curve (higher -> better) 
ROC_rf2_auc <- auc(ROC_rf2)
```

```{r}
# plot ROC curves 
plot(ROC_rf2, col = "red", main = "ROC For Random Forest, Default Settings")
lines(ROC_rf, col = "green")
# print the performance of each model 
paste("Accuracy % of default random forest: ", mean(heart_test$DEATH_EVENT == round(prediction[,2], digits = 0)))
paste("Area under curve of default random forest: ", ROC_rf_auc)

paste("Accuracy % of tuned random forest: ", mean(heart_test$DEATH_EVENT == round(prediction2[,2], digits = 0))) 
paste("Area under curve of tuned random forest: ", ROC_rf2_auc)
```

The accuracy and area under the receiver operating characteristic curve (AUC-ROC) are higher for the model produced using the default hyperparameters (88.3% and 0.932, respectively) compared to the "tuned" model (86.7% and 0.926, respectively).

The output of a Random Forest model is influenced by subsample size, the number of variables assessed at each node, the maximum nodesize of each leaf, and the number of predictors generated [@scornet2017tuning]. In R, the defaults are a subsample size of sqrt(𝑑) for classification (where d is the total number of dimensions), the generation of 500 predictors, and a nodesize of 5 [@scornet2017tuning]. In one comparison study, the balanced accuracy was found to be optimal at 1300 trees and 3 nodes in R and led to an increase of 0.09 (9%) in balanced accuracy as well as a 0.14 (14%) in the AUC-ROC compared to the values generated by the default hyperparameters [@lotsch2022biomedical].The results suggest the importance of tuning the hyperparameters of random forest algorithms, particularly in the case of exploratory analysis that has initial unfavorable accuracy. In the case of this dataset, the model with the default hyperparameters actually provided better results than the tuned model which implies that tuning is not always necessary or helpful for improving performance for the random forest algorithm.

### Statistical Modeling

## Conclusion


## References
